{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spark_rdd_and_df.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO9HKDKSYT/BK59+6MM1oXM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jugalpanchal/bd-chef/blob/main/spark_rdd_and_df.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5-JcY1SaVHd"
      },
      "source": [
        "# Follow the steps to install the dependencies:\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # install java\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz # spark package download\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz # unzip spark package\n",
        "!pip install -q findspark # install spark\n",
        "\n",
        "# Set the location of Java and Spark:\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUbmGLQVaeel"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import Row\n",
        "\n",
        "# create or get spark session\n",
        "spark = SparkSession.builder \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .appName(\"Spark_App1\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "# Spark 2.x does not give direct sparkcontext so we need to get it from SparkSession\n",
        "sc = spark.sparkContext\n",
        "#sc.appName #Spark_App1\n",
        "#sc.uiWebUrl # WebUI\n",
        "#sc.applicationId # application_xxxxxxxxxx_xxxx\n",
        "\n",
        "\n",
        "#sc: SparkContext\n",
        "  #1. play with RDD.\n",
        "  #2. It comes from the Spark-Core\n",
        "  #3. One only per application\n",
        "#spark: SparkSession\n",
        "  #1. play with DataFrame. \n",
        "  #2. It comes from the Spark-SQL.\n",
        "  #3. It merges SQLContext and HiveContext into one object.\n",
        "  #4. We can have multiple spark session objects in single application\n",
        "  #5. It allows us to access the SparkContext\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwtAMd19usN4"
      },
      "source": [
        "### RDD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_37d7reah0a",
        "outputId": "613dc1b0-f0bc-47dc-9e1f-73863bab734a"
      },
      "source": [
        "# A RDD is a collection of object of any type. It's schemaless.\n",
        "# There are three way we can create a RDD. 1. Using the parallelize method 2. Reading a file 3. From the another RDD\n",
        "# Read a file and create a RDD - RDD Started in Spark 1.x\n",
        "collection_rdd = sc.textFile(\"sample_data/anscombe.json\")\n",
        "#collection_rdd = sc.textFile(\"sample_data/*.json\") # multiple files\n",
        "\n",
        "# collection_rdd # sample_data/anscombe.json MapPartitionsRDD[260] at textFile at NativeMethodAccessorImpl.java:0\n",
        "collection_rdd.setName(\"anscombe_rdd\") # The Spark gives a internal name but we can specify the way we want. Even it helps in the SparkUI to identify specific RDD.\n",
        "collection_rdd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "anscombe_rdd MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUEvTm7itzMO"
      },
      "source": [
        "#collection_rdd.count() #49\n",
        "#collection_rdd.take(5) # top 5 entities"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_d_B3TMMbZ3M",
        "outputId": "90936229-3428-43c7-9163-d7b8971af761"
      },
      "source": [
        "collection_rdd.collect() # this is like a collection"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[',\n",
              " '  {\"Series\":\"I\", \"X\":10.0, \"Y\":8.04},',\n",
              " '  {\"Series\":\"I\", \"X\":8.0, \"Y\":6.95},',\n",
              " '  {\"Series\":\"I\", \"X\":13.0, \"Y\":7.58},',\n",
              " '  {\"Series\":\"I\", \"X\":9.0, \"Y\":8.81},',\n",
              " '  {\"Series\":\"I\", \"X\":11.0, \"Y\":8.33},',\n",
              " '  {\"Series\":\"I\", \"X\":14.0, \"Y\":9.96},',\n",
              " '  {\"Series\":\"I\", \"X\":6.0, \"Y\":7.24},',\n",
              " '  {\"Series\":\"I\", \"X\":4.0, \"Y\":4.26},',\n",
              " '  {\"Series\":\"I\", \"X\":12.0, \"Y\":10.84},',\n",
              " '  {\"Series\":\"I\", \"X\":7.0, \"Y\":4.81},',\n",
              " '  {\"Series\":\"I\", \"X\":5.0, \"Y\":5.68},',\n",
              " '',\n",
              " '  {\"Series\":\"II\", \"X\":10.0, \"Y\":9.14},',\n",
              " '  {\"Series\":\"II\", \"X\":8.0, \"Y\":8.14},',\n",
              " '  {\"Series\":\"II\", \"X\":13.0, \"Y\":8.74},',\n",
              " '  {\"Series\":\"II\", \"X\":9.0, \"Y\":8.77},',\n",
              " '  {\"Series\":\"II\", \"X\":11.0, \"Y\":9.26},',\n",
              " '  {\"Series\":\"II\", \"X\":14.0, \"Y\":8.10},',\n",
              " '  {\"Series\":\"II\", \"X\":6.0, \"Y\":6.13},',\n",
              " '  {\"Series\":\"II\", \"X\":4.0, \"Y\":3.10},',\n",
              " '  {\"Series\":\"II\", \"X\":12.0, \"Y\":9.13},',\n",
              " '  {\"Series\":\"II\", \"X\":7.0, \"Y\":7.26},',\n",
              " '  {\"Series\":\"II\", \"X\":5.0, \"Y\":4.74},',\n",
              " '',\n",
              " '  {\"Series\":\"III\", \"X\":10.0, \"Y\":7.46},',\n",
              " '  {\"Series\":\"III\", \"X\":8.0, \"Y\":6.77},',\n",
              " '  {\"Series\":\"III\", \"X\":13.0, \"Y\":12.74},',\n",
              " '  {\"Series\":\"III\", \"X\":9.0, \"Y\":7.11},',\n",
              " '  {\"Series\":\"III\", \"X\":11.0, \"Y\":7.81},',\n",
              " '  {\"Series\":\"III\", \"X\":14.0, \"Y\":8.84},',\n",
              " '  {\"Series\":\"III\", \"X\":6.0, \"Y\":6.08},',\n",
              " '  {\"Series\":\"III\", \"X\":4.0, \"Y\":5.39},',\n",
              " '  {\"Series\":\"III\", \"X\":12.0, \"Y\":8.15},',\n",
              " '  {\"Series\":\"III\", \"X\":7.0, \"Y\":6.42},',\n",
              " '  {\"Series\":\"III\", \"X\":5.0, \"Y\":5.73},',\n",
              " '',\n",
              " '  {\"Series\":\"IV\", \"X\":8.0, \"Y\":6.58},',\n",
              " '  {\"Series\":\"IV\", \"X\":8.0, \"Y\":5.76},',\n",
              " '  {\"Series\":\"IV\", \"X\":8.0, \"Y\":7.71},',\n",
              " '  {\"Series\":\"IV\", \"X\":8.0, \"Y\":8.84},',\n",
              " '  {\"Series\":\"IV\", \"X\":8.0, \"Y\":8.47},',\n",
              " '  {\"Series\":\"IV\", \"X\":8.0, \"Y\":7.04},',\n",
              " '  {\"Series\":\"IV\", \"X\":8.0, \"Y\":5.25},',\n",
              " '  {\"Series\":\"IV\", \"X\":19.0, \"Y\":12.50},',\n",
              " '  {\"Series\":\"IV\", \"X\":8.0, \"Y\":5.56},',\n",
              " '  {\"Series\":\"IV\", \"X\":8.0, \"Y\":7.91},',\n",
              " '  {\"Series\":\"IV\", \"X\":8.0, \"Y\":6.89}',\n",
              " ']']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQVAnEWsuvX2"
      },
      "source": [
        "### DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsOowOUJak8W",
        "outputId": "e1ec2190-ccb7-498d-ffcf-ae28aa3cc600"
      },
      "source": [
        "# Read a file and create a DataFrame - DataFrame Started in Spark 2.x but on top of the RDD.\n",
        "# Note: The loading(spark.read.json) the file into the dataframe is not lazy-loading like other transformation.\n",
        "# But the RDD has lazy-loading. The dataframe inferes the schema so it needs to have eager loading.\n",
        "json_df = spark.read.json('sample_data/anscombe.json') # first row is always a columns row.\n",
        "json_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Series: string, X: double, Y: double, _corrupt_record: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkNpX4Uxbpyc",
        "outputId": "04164611-61ff-4f24-b849-2aa65031abdc"
      },
      "source": [
        "json_df.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Series: string (nullable = true)\n",
            " |-- X: double (nullable = true)\n",
            " |-- Y: double (nullable = true)\n",
            " |-- _corrupt_record: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sa0oFO4hbf1e",
        "outputId": "e51b81b5-e6e0-4030-e255-576a49ffc16c"
      },
      "source": [
        "json_df.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(Series=None, X=None, Y=None, _corrupt_record='['),\n",
              " Row(Series='I', X=10.0, Y=8.04, _corrupt_record=None),\n",
              " Row(Series='I', X=8.0, Y=6.95, _corrupt_record=None),\n",
              " Row(Series='I', X=13.0, Y=7.58, _corrupt_record=None),\n",
              " Row(Series='I', X=9.0, Y=8.81, _corrupt_record=None),\n",
              " Row(Series='I', X=11.0, Y=8.33, _corrupt_record=None),\n",
              " Row(Series='I', X=14.0, Y=9.96, _corrupt_record=None),\n",
              " Row(Series='I', X=6.0, Y=7.24, _corrupt_record=None),\n",
              " Row(Series='I', X=4.0, Y=4.26, _corrupt_record=None),\n",
              " Row(Series='I', X=12.0, Y=10.84, _corrupt_record=None),\n",
              " Row(Series='I', X=7.0, Y=4.81, _corrupt_record=None),\n",
              " Row(Series='I', X=5.0, Y=5.68, _corrupt_record=None),\n",
              " Row(Series='II', X=10.0, Y=9.14, _corrupt_record=None),\n",
              " Row(Series='II', X=8.0, Y=8.14, _corrupt_record=None),\n",
              " Row(Series='II', X=13.0, Y=8.74, _corrupt_record=None),\n",
              " Row(Series='II', X=9.0, Y=8.77, _corrupt_record=None),\n",
              " Row(Series='II', X=11.0, Y=9.26, _corrupt_record=None),\n",
              " Row(Series='II', X=14.0, Y=8.1, _corrupt_record=None),\n",
              " Row(Series='II', X=6.0, Y=6.13, _corrupt_record=None),\n",
              " Row(Series='II', X=4.0, Y=3.1, _corrupt_record=None),\n",
              " Row(Series='II', X=12.0, Y=9.13, _corrupt_record=None),\n",
              " Row(Series='II', X=7.0, Y=7.26, _corrupt_record=None),\n",
              " Row(Series='II', X=5.0, Y=4.74, _corrupt_record=None),\n",
              " Row(Series='III', X=10.0, Y=7.46, _corrupt_record=None),\n",
              " Row(Series='III', X=8.0, Y=6.77, _corrupt_record=None),\n",
              " Row(Series='III', X=13.0, Y=12.74, _corrupt_record=None),\n",
              " Row(Series='III', X=9.0, Y=7.11, _corrupt_record=None),\n",
              " Row(Series='III', X=11.0, Y=7.81, _corrupt_record=None),\n",
              " Row(Series='III', X=14.0, Y=8.84, _corrupt_record=None),\n",
              " Row(Series='III', X=6.0, Y=6.08, _corrupt_record=None),\n",
              " Row(Series='III', X=4.0, Y=5.39, _corrupt_record=None),\n",
              " Row(Series='III', X=12.0, Y=8.15, _corrupt_record=None),\n",
              " Row(Series='III', X=7.0, Y=6.42, _corrupt_record=None),\n",
              " Row(Series='III', X=5.0, Y=5.73, _corrupt_record=None),\n",
              " Row(Series='IV', X=8.0, Y=6.58, _corrupt_record=None),\n",
              " Row(Series='IV', X=8.0, Y=5.76, _corrupt_record=None),\n",
              " Row(Series='IV', X=8.0, Y=7.71, _corrupt_record=None),\n",
              " Row(Series='IV', X=8.0, Y=8.84, _corrupt_record=None),\n",
              " Row(Series='IV', X=8.0, Y=8.47, _corrupt_record=None),\n",
              " Row(Series='IV', X=8.0, Y=7.04, _corrupt_record=None),\n",
              " Row(Series='IV', X=8.0, Y=5.25, _corrupt_record=None),\n",
              " Row(Series='IV', X=19.0, Y=12.5, _corrupt_record=None),\n",
              " Row(Series='IV', X=8.0, Y=5.56, _corrupt_record=None),\n",
              " Row(Series='IV', X=8.0, Y=7.91, _corrupt_record=None),\n",
              " Row(Series='IV', X=8.0, Y=6.89, _corrupt_record=None),\n",
              " Row(Series=None, X=None, Y=None, _corrupt_record=']')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZXXKddKx0hC",
        "outputId": "38d2ecfc-eca5-40a5-f9a0-f53fc93b38dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "json_df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+----+-----+---------------+\n",
            "|Series|   X|    Y|_corrupt_record|\n",
            "+------+----+-----+---------------+\n",
            "|  null|null| null|              [|\n",
            "|     I|10.0| 8.04|           null|\n",
            "|     I| 8.0| 6.95|           null|\n",
            "|     I|13.0| 7.58|           null|\n",
            "|     I| 9.0| 8.81|           null|\n",
            "|     I|11.0| 8.33|           null|\n",
            "|     I|14.0| 9.96|           null|\n",
            "|     I| 6.0| 7.24|           null|\n",
            "|     I| 4.0| 4.26|           null|\n",
            "|     I|12.0|10.84|           null|\n",
            "|     I| 7.0| 4.81|           null|\n",
            "|     I| 5.0| 5.68|           null|\n",
            "|    II|10.0| 9.14|           null|\n",
            "|    II| 8.0| 8.14|           null|\n",
            "|    II|13.0| 8.74|           null|\n",
            "|    II| 9.0| 8.77|           null|\n",
            "|    II|11.0| 9.26|           null|\n",
            "|    II|14.0|  8.1|           null|\n",
            "|    II| 6.0| 6.13|           null|\n",
            "|    II| 4.0|  3.1|           null|\n",
            "+------+----+-----+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vpVtYVAu5PE"
      },
      "source": [
        "### RDD to DataFrame and vice versa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkY73YXpu8BO"
      },
      "source": [
        "generic_data_rdd = sc.parallelize([1, 'Alice', 50, 'Bob', 'Canton']) # RDD collection supports different type of items even in a nested colletion.\n",
        "#generic_data_rdd.toDF() # It does not work because it contains object with differnt types."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezAapIlKwzMi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c17d7491-76a1-4942-a9d1-fc7ab146aef1"
      },
      "source": [
        "people_list = [('Alice', 25, 1500, 15), ('Bob', 20, 1000, 10), ('Charly', 30, 2000, 20), ('Don', 15, 500, 5), ('Eric', 20, 1500, 15)]\n",
        "people_rdd = sc.parallelize(people_list) # create a RDD but with Tuple/Pair objects\n",
        "#people_rdd # ParallelCollectionRDD[2] at readRDDFromFile\n",
        "\n",
        "people_rdd.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Alice', 25, 1500, 15),\n",
              " ('Bob', 20, 1000, 10),\n",
              " ('Charly', 30, 2000, 20),\n",
              " ('Don', 15, 500, 5),\n",
              " ('Eric', 20, 1500, 15)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G26PuH0xzoj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a51b122-8e68-4a43-9d5a-2b2d0f66c487"
      },
      "source": [
        "people_without_col_df = people_rdd.toDF() # Column names have been automatically generated and assigned. It infers data types too.\n",
        "#people_without_col_df # DataFrame[_1: string, _2: bigint]\n",
        "people_without_col_df.collect() # shows data with its class"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(_1='Alice', _2=25, _3=1500, _4=15),\n",
              " Row(_1='Bob', _2=20, _3=1000, _4=10),\n",
              " Row(_1='Charly', _2=30, _3=2000, _4=20),\n",
              " Row(_1='Don', _2=15, _3=500, _4=5),\n",
              " Row(_1='Eric', _2=20, _3=1500, _4=15)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyTXqJ63zQmI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "850c8321-07db-4e44-9a1f-4ffe03575b4c"
      },
      "source": [
        "people_without_col_df.show() # shows in tabular format"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+---+----+---+\n",
            "|    _1| _2|  _3| _4|\n",
            "+------+---+----+---+\n",
            "| Alice| 25|1500| 15|\n",
            "|   Bob| 20|1000| 10|\n",
            "|Charly| 30|2000| 20|\n",
            "|   Don| 15| 500|  5|\n",
            "|  Eric| 20|1500| 15|\n",
            "+------+---+----+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrXYm9SLyXup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "992a110e-31d5-40b7-cee0-3fedabd1018a"
      },
      "source": [
        "from pyspark import Row\n",
        "# There are two way to get custom column names.\n",
        "# 1. Create the row class object while creating a RDD\n",
        "data_row_rdd = sc.parallelize([Row(name='Alice', age=50), \n",
        "                               Row(name='Bob', age=20)])\n",
        "data_row_rdd.collect() # [Row(name='Alice', age=50), Row(name='Bob', age=20)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(name='Alice', age=50), Row(name='Bob', age=20)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4E_xlfUK1fDn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07e3ead1-8d56-4e92-ff0c-0aa0e29dca5b"
      },
      "source": [
        "# 2. Transform the existing RDD to the new RDD with the row object and column names.\n",
        "people_row_rdd = people_rdd.map(lambda x: \n",
        "                                      Row(\n",
        "                                          name=x[0], \n",
        "                                          age=int(x[1]), \n",
        "                                          salary=int(x[2]),\n",
        "                                          tax=int(x[3])\n",
        "                                          )\n",
        "                                     ) # create a RDD with Row objects\n",
        "people_row_rdd # PythonRDD[63] at RDD\n",
        "people_row_rdd.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(name='Alice', age=25, salary=1500, tax=15),\n",
              " Row(name='Bob', age=20, salary=1000, tax=10),\n",
              " Row(name='Charly', age=30, salary=2000, tax=20),\n",
              " Row(name='Don', age=15, salary=500, tax=5),\n",
              " Row(name='Eric', age=20, salary=1500, tax=15)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZpnYejtx8Ly",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60d497c9-c052-4b00-db65-c89933ff169c"
      },
      "source": [
        "data_row_rdd.toDF().show()\n",
        "people_row_rdd.toDF().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+---+\n",
            "| name|age|\n",
            "+-----+---+\n",
            "|Alice| 50|\n",
            "|  Bob| 20|\n",
            "+-----+---+\n",
            "\n",
            "+------+---+------+---+\n",
            "|  name|age|salary|tax|\n",
            "+------+---+------+---+\n",
            "| Alice| 25|  1500| 15|\n",
            "|   Bob| 20|  1000| 10|\n",
            "|Charly| 30|  2000| 20|\n",
            "|   Don| 15|   500|  5|\n",
            "|  Eric| 20|  1500| 15|\n",
            "+------+---+------+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXbgAOjI3hdv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e133d6c-c0a8-4971-fd31-1d48dafc8625"
      },
      "source": [
        "people_without_col_df.rdd # Every DataFrame contains a RDD and we can get the dataframe's RDD easily."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MapPartitionsRDD[43] at javaToPython at NativeMethodAccessorImpl.java:0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKBZpCu8-lrV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7508f7f5-6ccf-49ae-ee87-1d969be9a30c"
      },
      "source": [
        "people_df = people_row_rdd.toDF()\n",
        "people_df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+---+------+---+\n",
            "|  name|age|salary|tax|\n",
            "+------+---+------+---+\n",
            "| Alice| 25|  1500| 15|\n",
            "|   Bob| 20|  1000| 10|\n",
            "|Charly| 30|  2000| 20|\n",
            "|   Don| 15|   500|  5|\n",
            "|  Eric| 20|  1500| 15|\n",
            "+------+---+------+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjqGQ82VerD3",
        "outputId": "d8d12aef-773c-4ce8-db4f-e1c3f0109920"
      },
      "source": [
        "# Create a dataframe\n",
        "names_df = spark.createDataFrame([(1, 'Jugal'), (2, 'Garvik')])\n",
        "#names_df # DataFrame[_1: bigint, _2: string]\n",
        "\n",
        "#names_df.collect()\n",
        "# [Row(_1=1, _2='Jugal'), Row(_1=2, _2='Garvik')]\n",
        "\n",
        "#names_df.take(1) # only 1 rows\n",
        "# [Row(_1=1, _2='Jugal')]\n",
        "\n",
        "#names_df.show()\n",
        "#+---+------+\n",
        "#| _1|    _2|\n",
        "#+---+------+\n",
        "#|  1| Jugal|\n",
        "#|  2|Garvik|\n",
        "#+---+------+\n",
        "\n",
        "#names_df.limit(1).show()\n",
        "#+---+-----+\n",
        "#| _1|   _2|\n",
        "#+---+-----+\n",
        "#|  1|Jugal|\n",
        "#+---+-----+\n",
        "\n",
        "# Note: _1 and _2 are two column names.\n",
        "\n",
        "#names_with_schema_df = names_df.toDF(\"Id\", \"Name\") # the toDF() is also avaiable with the dataframe to specify the schema.\n",
        "#names_with_schema_df.show()\n",
        "#+---+------+\n",
        "#| Id|  Name|\n",
        "#+---+------+\n",
        "#|  1| Jugal|\n",
        "#|  2|Garvik|\n",
        "#+---+------+\n",
        "\n",
        "#names_df.printSchema()\n",
        "#root\n",
        "# |-- _1: long (nullable = true)\n",
        "# |-- _2: string (nullable = true)\n",
        "\n",
        "#names_with_schema_df.printSchema()\n",
        "#root\n",
        "# |-- Id: long (nullable = true)\n",
        "# |-- Name: string (nullable = true)\n",
        "\n",
        "names_rdd = names_df.rdd\n",
        "names_rdd.collect() # It contains the row objects."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(_1=1, _2='Jugal'), Row(_1=2, _2='Garvik')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uf63F5uN2NEB"
      },
      "source": [
        "### Transformation on DataFrame\n",
        "####FlatMap, Map, ReduceByKey"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hymbpY6m2hh1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8772d65-86b2-4bf6-a921-f3d93520ccb7"
      },
      "source": [
        "# combining - within partition\n",
        "# merging - across partition\n",
        "\n",
        "# map operation on RDD\n",
        "people_rdd = people_df.rdd.map(lambda x: (x.age, x.age + 5))\n",
        "#people_rdd # PythonRDD[78] at RDD\n",
        "people_rdd.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(25, 30), (20, 25), (30, 35), (15, 20), (20, 25)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B07N6Uvi5N6B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bf58bd6-45e0-49ae-edf7-2768741d1a43"
      },
      "source": [
        "# Map: the map operation performs a transformation on every element in the RDD. \n",
        "# Similar kind of method is not available for the DataFrame, \n",
        "# so we compute and assign computed value in new column within the DataFrame. \n",
        "\n",
        "people_df_new_age = people_df.select('name', 'age').withColumn('new_age', people_df.age + 10) # we can type people_df.age to mention column name.\n",
        "people_df_new_age.show()\n",
        "\n",
        "people_df_new_age = people_df_new_age.withColumnRenamed('new_age', 'after_ten_years')\n",
        "people_df_new_age = people_df_new_age.drop('age')\n",
        "people_df_new_age.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+---+-------+\n",
            "|  name|age|new_age|\n",
            "+------+---+-------+\n",
            "| Alice| 25|     35|\n",
            "|   Bob| 20|     30|\n",
            "|Charly| 30|     40|\n",
            "|   Don| 15|     25|\n",
            "|  Eric| 20|     30|\n",
            "+------+---+-------+\n",
            "\n",
            "+------+---------------+\n",
            "|  name|after_ten_years|\n",
            "+------+---------------+\n",
            "| Alice|             35|\n",
            "|   Bob|             30|\n",
            "|Charly|             40|\n",
            "|   Don|             25|\n",
            "|  Eric|             30|\n",
            "+------+---------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEfggXez5GNz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43851de3-eb90-4a3d-d886-913bce797682"
      },
      "source": [
        "people_df.first()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(name='Alice', age=25, salary=1500, tax=15)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecH5aObO2hqE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d63ddf6-f0ef-4100-c7ca-017e89164398"
      },
      "source": [
        "people_df.take(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(name='Alice', age=25, salary=1500, tax=15),\n",
              " Row(name='Bob', age=20, salary=1000, tax=10)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61qXp_Ja2uZM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "77e9df3d-e843-43aa-bcae-42500e3b17be"
      },
      "source": [
        "people_df.collect()[1][0] # It is a tabular format so can access specific cell.\n",
        "# It always creates a clone then return so if the cell has any class type and we modify then it won't change in the dataframe."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Bob'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4W9Y2c3_Maq",
        "outputId": "55293134-a5c5-461d-bae2-3a1fd93cf5b9"
      },
      "source": [
        "\"\"\"\n",
        "Text in the datasets/simple_titles.txt:\n",
        "How can I use DataFrames in 2.0\n",
        "What is an RDD and Schema RDD\n",
        "How do I group by a field\n",
        "Can I use Hive from HUE\n",
        "\"\"\"\n",
        "\n",
        "# read lines\n",
        "lines_rdd = sc.textFile(\"datasets/simple_titles.txt\")\n",
        "print('lines', lines_rdd.collect())\n",
        "\n",
        "# flat all words\n",
        "words_rdd = lines_rdd.flatMap(lambda line: line.split(' '))\n",
        "print('words', words_rdd.collect())\n",
        "\n",
        "# map each word with 1 value\n",
        "word_and_count_rdd = words_rdd.map(lambda x: (x,1))\n",
        "print('word_for_count', word_and_count_rdd.collect())\n",
        "\n",
        "# count each word by key\n",
        "word_with_count_rdd = word_and_count_rdd.reduceByKey(lambda x,y: x + y)\n",
        "print('word_with_count_rdd', word_with_count_rdd.collect())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lines ['Text in the datasets/simple_titles.txt:', 'How can I use DataFrames in 2.0', 'What is an RDD and Schema RDD', 'How do I group by a field', 'Can I use Hive from HUE']\n",
            "words ['Text', 'in', 'the', 'datasets/simple_titles.txt:', 'How', 'can', 'I', 'use', 'DataFrames', 'in', '2.0', 'What', 'is', 'an', 'RDD', 'and', 'Schema', 'RDD', 'How', 'do', 'I', 'group', 'by', 'a', 'field', 'Can', 'I', 'use', 'Hive', 'from', 'HUE']\n",
            "word_for_count [('Text', 1), ('in', 1), ('the', 1), ('datasets/simple_titles.txt:', 1), ('How', 1), ('can', 1), ('I', 1), ('use', 1), ('DataFrames', 1), ('in', 1), ('2.0', 1), ('What', 1), ('is', 1), ('an', 1), ('RDD', 1), ('and', 1), ('Schema', 1), ('RDD', 1), ('How', 1), ('do', 1), ('I', 1), ('group', 1), ('by', 1), ('a', 1), ('field', 1), ('Can', 1), ('I', 1), ('use', 1), ('Hive', 1), ('from', 1), ('HUE', 1)]\n",
            "word_with_count_rdd [('in', 2), ('use', 2), ('DataFrames', 1), ('2.0', 1), ('What', 1), ('is', 1), ('an', 1), ('do', 1), ('group', 1), ('field', 1), ('Hive', 1), ('Text', 1), ('the', 1), ('datasets/simple_titles.txt:', 1), ('How', 2), ('can', 1), ('I', 3), ('RDD', 2), ('and', 1), ('Schema', 1), ('by', 1), ('a', 1), ('Can', 1), ('from', 1), ('HUE', 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_-2qtG4g6x7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "076ec9b2-7674-437d-b69f-3fdfd5a608a7"
      },
      "source": [
        "dir(people_df) # check all the methods which can be used with the RDD."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattr__',\n",
              " '__getattribute__',\n",
              " '__getitem__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_collect_as_arrow',\n",
              " '_jcols',\n",
              " '_jdf',\n",
              " '_jmap',\n",
              " '_jseq',\n",
              " '_lazy_rdd',\n",
              " '_repr_html_',\n",
              " '_sc',\n",
              " '_schema',\n",
              " '_sort_cols',\n",
              " '_support_repr_html',\n",
              " '_to_corrected_pandas_type',\n",
              " 'agg',\n",
              " 'alias',\n",
              " 'approxQuantile',\n",
              " 'cache',\n",
              " 'checkpoint',\n",
              " 'coalesce',\n",
              " 'colRegex',\n",
              " 'collect',\n",
              " 'columns',\n",
              " 'corr',\n",
              " 'count',\n",
              " 'cov',\n",
              " 'createGlobalTempView',\n",
              " 'createOrReplaceGlobalTempView',\n",
              " 'createOrReplaceTempView',\n",
              " 'createTempView',\n",
              " 'crossJoin',\n",
              " 'crosstab',\n",
              " 'cube',\n",
              " 'describe',\n",
              " 'distinct',\n",
              " 'drop',\n",
              " 'dropDuplicates',\n",
              " 'drop_duplicates',\n",
              " 'dropna',\n",
              " 'dtypes',\n",
              " 'exceptAll',\n",
              " 'explain',\n",
              " 'fillna',\n",
              " 'filter',\n",
              " 'first',\n",
              " 'foreach',\n",
              " 'foreachPartition',\n",
              " 'freqItems',\n",
              " 'groupBy',\n",
              " 'groupby',\n",
              " 'head',\n",
              " 'hint',\n",
              " 'inputFiles',\n",
              " 'intersect',\n",
              " 'intersectAll',\n",
              " 'isLocal',\n",
              " 'isStreaming',\n",
              " 'is_cached',\n",
              " 'join',\n",
              " 'limit',\n",
              " 'localCheckpoint',\n",
              " 'mapInPandas',\n",
              " 'na',\n",
              " 'orderBy',\n",
              " 'persist',\n",
              " 'printSchema',\n",
              " 'randomSplit',\n",
              " 'rdd',\n",
              " 'registerTempTable',\n",
              " 'repartition',\n",
              " 'repartitionByRange',\n",
              " 'replace',\n",
              " 'rollup',\n",
              " 'sameSemantics',\n",
              " 'sample',\n",
              " 'sampleBy',\n",
              " 'schema',\n",
              " 'select',\n",
              " 'selectExpr',\n",
              " 'semanticHash',\n",
              " 'show',\n",
              " 'sort',\n",
              " 'sortWithinPartitions',\n",
              " 'sql_ctx',\n",
              " 'stat',\n",
              " 'storageLevel',\n",
              " 'subtract',\n",
              " 'summary',\n",
              " 'tail',\n",
              " 'take',\n",
              " 'toDF',\n",
              " 'toJSON',\n",
              " 'toLocalIterator',\n",
              " 'toPandas',\n",
              " 'transform',\n",
              " 'union',\n",
              " 'unionAll',\n",
              " 'unionByName',\n",
              " 'unpersist',\n",
              " 'where',\n",
              " 'withColumn',\n",
              " 'withColumnRenamed',\n",
              " 'withWatermark',\n",
              " 'write',\n",
              " 'writeStream',\n",
              " 'writeTo']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_jOgc3Bi5ez",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "432320ca-9291-4570-b3c6-57fd2cc32228"
      },
      "source": [
        "#help(people_df) # even better with example but it gives details about all the methods.\n",
        "help(people_df.collect) # for specific method"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on method collect in module pyspark.sql.dataframe:\n",
            "\n",
            "collect() method of pyspark.sql.dataframe.DataFrame instance\n",
            "    Returns all the records as a list of :class:`Row`.\n",
            "    \n",
            "    .. versionadded:: 1.3.0\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    >>> df.collect()\n",
            "    [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsTXb38aLxI6"
      },
      "source": [
        "### Filter, Select, When\n",
        "\n",
        "#### The where is the Filter alias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-STLJOgsMj5M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d30cca67-a409-4586-85f0-ef65dfdc8533"
      },
      "source": [
        "from pyspark.sql.functions import col, lit, concat\n",
        "from pyspark.sql.functions import when\n",
        "people_df.show()\n",
        "\n",
        "# 1. Canonical Notation\n",
        "people_df['name']\n",
        "people_df['Name']\n",
        "\n",
        "# 2. Dot(.) Notation\n",
        "people_df.name\n",
        "#people_df.Name #AttributeError: 'DataFrame' object has no attribute 'Name'\n",
        "\n",
        "# 3. Col Notation\n",
        "#people_df[col['name']]\n",
        "\n",
        "# 4. SQL Notation\n",
        "#'name'\n",
        "\n",
        "# Specify a constant with the lit(literal) otherwise it is considered as a column name.\n",
        "people_df.select(people_df['Name'], people_df.name, col('name'), 'name', concat(lit('Std '), col('name')) , people_df.age + 10).show()\n",
        "#people_df.select(col['name']).show()\n",
        "\n",
        "people_df.select('name', when(col('age') > 30, 'Adult').otherwise('Young')).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+---+------+---+\n",
            "|  name|age|salary|tax|\n",
            "+------+---+------+---+\n",
            "| Alice| 25|  1500| 15|\n",
            "|   Bob| 20|  1000| 10|\n",
            "|Charly| 30|  2000| 20|\n",
            "|   Don| 15|   500|  5|\n",
            "|  Eric| 20|  1500| 15|\n",
            "+------+---+------+---+\n",
            "\n",
            "+------+------+------+------+------------------+----------+\n",
            "|  Name|  name|  name|  name|concat(Std , name)|(age + 10)|\n",
            "+------+------+------+------+------------------+----------+\n",
            "| Alice| Alice| Alice| Alice|         Std Alice|        35|\n",
            "|   Bob|   Bob|   Bob|   Bob|           Std Bob|        30|\n",
            "|Charly|Charly|Charly|Charly|        Std Charly|        40|\n",
            "|   Don|   Don|   Don|   Don|           Std Don|        25|\n",
            "|  Eric|  Eric|  Eric|  Eric|          Std Eric|        30|\n",
            "+------+------+------+------+------------------+----------+\n",
            "\n",
            "+------+----------------------------------------------+\n",
            "|  name|CASE WHEN (age > 30) THEN Adult ELSE Young END|\n",
            "+------+----------------------------------------------+\n",
            "| Alice|                                         Young|\n",
            "|   Bob|                                         Young|\n",
            "|Charly|                                         Young|\n",
            "|   Don|                                         Young|\n",
            "|  Eric|                                         Young|\n",
            "+------+----------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysMbKLHyhvm7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a1f177e-3bd4-4407-fbdf-c28bd343909b"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "people_df.filter(F.col(\"name\").like(\"%Bob%\")).show() # the name contains Bob."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+---+------+---+\n",
            "|name|age|salary|tax|\n",
            "+----+---+------+---+\n",
            "| Bob| 20|  1000| 10|\n",
            "+----+---+------+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlUft-WNl1ym"
      },
      "source": [
        "### OrderBy, GroupBy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRmh78Cml2At",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f63b2f18-5853-457c-c964-c1f83968a463"
      },
      "source": [
        "people_df.orderBy(F.col(\"name\"), ascending=False).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+---+------+---+\n",
            "|  name|age|salary|tax|\n",
            "+------+---+------+---+\n",
            "|  Eric| 20|  1500| 15|\n",
            "|   Don| 15|   500|  5|\n",
            "|Charly| 30|  2000| 20|\n",
            "|   Bob| 20|  1000| 10|\n",
            "| Alice| 25|  1500| 15|\n",
            "+------+---+------+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqhrN8V6oFSM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6bf0674-f2b8-43e8-d38c-09ac9c5c482a"
      },
      "source": [
        "people_df.groupBy(\"age\").count().show() # age count\n",
        "people_df.groupBy(\"age\").max(\"salary\").show() # max salary of each age\n",
        "\n",
        "# Multiple aggregation functions\n",
        "people_df.groupBy(\"age\")\\\n",
        "        .agg(\n",
        "            F.min(\"salary\"),\n",
        "            F.max(\"salary\"),\n",
        "            F.avg(\"salary\"),\n",
        "            F.sum(\"salary\"),\n",
        "            F.sum(\"tax\")\n",
        "            )\\\n",
        "        .show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-----+\n",
            "|age|count|\n",
            "+---+-----+\n",
            "| 25|    1|\n",
            "| 15|    1|\n",
            "| 30|    1|\n",
            "| 20|    2|\n",
            "+---+-----+\n",
            "\n",
            "+---+-----------+\n",
            "|age|max(salary)|\n",
            "+---+-----------+\n",
            "| 25|       1500|\n",
            "| 15|        500|\n",
            "| 30|       2000|\n",
            "| 20|       1500|\n",
            "+---+-----------+\n",
            "\n",
            "+---+-----------+-----------+-----------+-----------+--------+\n",
            "|age|min(salary)|max(salary)|avg(salary)|sum(salary)|sum(tax)|\n",
            "+---+-----------+-----------+-----------+-----------+--------+\n",
            "| 25|       1500|       1500|     1500.0|       1500|      15|\n",
            "| 15|        500|        500|      500.0|        500|       5|\n",
            "| 30|       2000|       2000|     2000.0|       2000|      20|\n",
            "| 20|       1000|       1500|     1250.0|       2500|      25|\n",
            "+---+-----------+-----------+-----------+-----------+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "people_df.groupBy('age').count().filter(F.col('count') > 1).show() # the having clause"
      ],
      "metadata": {
        "id": "4J2W5BoRFyTS",
        "outputId": "2f76abaf-6830-4311-ebc3-2efb668541d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0b85825f7c19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpeople_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'age'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'people_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juSAc5ZbPxlo"
      },
      "source": [
        "###null"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc9Pf-5PP711",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "880cfc14-0dfb-45a9-f960-308f9a7393fe"
      },
      "source": [
        "# Drop all rows if any column has null.\n",
        "people_df.select(\"name\", \"age\").dropna().show()\n",
        "\n",
        "# Parameters: any, how, subset\n",
        "people_df.select(\"name\", \"age\").dropna(how='any')\n",
        "people_df.select(\"name\", \"age\").dropna(how='all')\n",
        "people_df.select(\"name\", \"age\").dropna(subset='age')\n",
        "\n",
        "# Replace null with any value.\n",
        "people_df.select(\"name\", \"age\").fillna('[N/A]')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+---+\n",
            "|  name|age|\n",
            "+------+---+\n",
            "| Alice| 25|\n",
            "|   Bob| 20|\n",
            "|Charly| 30|\n",
            "|   Don| 15|\n",
            "|  Eric| 20|\n",
            "+------+---+\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[name: string, age: bigint]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HDJJOPwCcn7"
      },
      "source": [
        "###Explode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9PeiQLPCfMG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5269e734-e167-43fa-880d-884389d96a7b"
      },
      "source": [
        "# PySpark explode function can be used to explode an Array of Array (nested Array) ArrayType(ArrayType(StringType)) columns to rows.\n",
        "\n",
        "from pyspark.sql.functions import explode\n",
        "\n",
        "veg_list_df = spark.createDataFrame([(['a', 'b', 'c'], 1), (['b', 'd'], 2)])\n",
        "veg_list_df.show()\n",
        "\n",
        "veg_list_df.select(explode(veg_list_df._1), veg_list_df._2).show()\n",
        "\n",
        "veg_list_df.select(explode(veg_list_df._1), veg_list_df._2).distinct().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+\n",
            "|       _1| _2|\n",
            "+---------+---+\n",
            "|[a, b, c]|  1|\n",
            "|   [b, d]|  2|\n",
            "+---------+---+\n",
            "\n",
            "+---+---+\n",
            "|col| _2|\n",
            "+---+---+\n",
            "|  a|  1|\n",
            "|  b|  1|\n",
            "|  c|  1|\n",
            "|  b|  2|\n",
            "|  d|  2|\n",
            "+---+---+\n",
            "\n",
            "+---+---+\n",
            "|col| _2|\n",
            "+---+---+\n",
            "|  a|  1|\n",
            "|  b|  1|\n",
            "|  d|  2|\n",
            "|  c|  1|\n",
            "|  b|  2|\n",
            "+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arrayArrayData = [\n",
        "  (\"James\",[[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"]]),\n",
        "  (\"Michael\",[[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"]]),\n",
        "  (\"Robert\",[[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"]])\n",
        "]\n",
        "\n",
        "df_arrayofarray = spark.createDataFrame(data=arrayArrayData, schema = ['name','subjects_array'])\n",
        "df_arrayofarray.printSchema()\n",
        "df_arrayofarray.show(truncate=False)\n",
        "\n",
        "df_array1 = df_arrayofarray.withColumn(\"df_array1\", explode(df_arrayofarray.subjects_array).alias('subject_array'))\n",
        "df_array1.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qu3aWNHmJuwR",
        "outputId": "382206c0-7e7d-45e3-d06e-c39d56c61717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- subjects_array: array (nullable = true)\n",
            " |    |-- element: array (containsNull = true)\n",
            " |    |    |-- element: string (containsNull = true)\n",
            "\n",
            "+-------+-----------------------------------+\n",
            "|name   |subjects_array                     |\n",
            "+-------+-----------------------------------+\n",
            "|James  |[[Java, Scala, C++], [Spark, Java]]|\n",
            "|Michael|[[Spark, Java, C++], [Spark, Java]]|\n",
            "|Robert |[[CSharp, VB], [Spark, Python]]    |\n",
            "+-------+-----------------------------------+\n",
            "\n",
            "+-------+-----------------------------------+------------------+\n",
            "|name   |subjects_array                     |df_array1         |\n",
            "+-------+-----------------------------------+------------------+\n",
            "|James  |[[Java, Scala, C++], [Spark, Java]]|[Java, Scala, C++]|\n",
            "|James  |[[Java, Scala, C++], [Spark, Java]]|[Spark, Java]     |\n",
            "|Michael|[[Spark, Java, C++], [Spark, Java]]|[Spark, Java, C++]|\n",
            "|Michael|[[Spark, Java, C++], [Spark, Java]]|[Spark, Java]     |\n",
            "|Robert |[[CSharp, VB], [Spark, Python]]    |[CSharp, VB]      |\n",
            "|Robert |[[CSharp, VB], [Spark, Python]]    |[Spark, Python]   |\n",
            "+-------+-----------------------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Flatten"
      ],
      "metadata": {
        "id": "intbOWgZLxTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If you want to flatten the arrays, use flatten function which converts array of array columns to a single array on DataFrame.\n",
        "from pyspark.sql.functions import flatten\n",
        "\n",
        "df_arrayofarray.show(truncate=False)\n",
        "df_array2 = df_arrayofarray.select(df_arrayofarray.name, flatten(df_arrayofarray.subjects_array))\n",
        "df_array2.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZzxFqX5L0bl",
        "outputId": "8d998548-6c97-4a55-dcd5-0e1386c8ba09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------------------------------+\n",
            "|name   |subjects_array                     |\n",
            "+-------+-----------------------------------+\n",
            "|James  |[[Java, Scala, C++], [Spark, Java]]|\n",
            "|Michael|[[Spark, Java, C++], [Spark, Java]]|\n",
            "|Robert |[[CSharp, VB], [Spark, Python]]    |\n",
            "+-------+-----------------------------------+\n",
            "\n",
            "+-------+-------------------------------+\n",
            "|name   |flatten(subjects_array)        |\n",
            "+-------+-------------------------------+\n",
            "|James  |[Java, Scala, C++, Spark, Java]|\n",
            "|Michael|[Spark, Java, C++, Spark, Java]|\n",
            "|Robert |[CSharp, VB, Spark, Python]    |\n",
            "+-------+-------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_vZ3WzLdKdV"
      },
      "source": [
        "###Partition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQtZZ1uNdMYy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbed8917-3304-4fae-9565-8a81c1b59b78"
      },
      "source": [
        "list1_rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
        "#list1_rdd # ParallelCollectionRDD[1] at readRDDFromFile at PythonRDD.scala:274\n",
        "#list1_rdd.collect() # [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "#list1_rdd.getNumPartitions() # 2 - even though it is small data it has created 2 partition.\n",
        "partitions_array = list1_rdd.glom() # gluam method bring all the partition as individual partition. don't try with big valume data.\n",
        "partitions_array.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2, 3, 4], [5, 6, 7, 8, 9]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHtdeIFufMYa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddcd9648-67d5-48e5-8f6b-6e50e79d1a3a"
      },
      "source": [
        "list2_rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9], 3) # specify partition explicitly\n",
        "list2_rdd.getNumPartitions() # 3\n",
        "partitions_array = list2_rdd.glom()\n",
        "partitions_array.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2, 3], [4, 5, 6], [7, 8, 9]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUebOIFU-hFl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36ae31bb-9f17-41b9-c83e-8c7e7ae6137a"
      },
      "source": [
        "list3_rdd = list2_rdd.repartition(2) # specifies new partition, up or down but there would be shuffling.\n",
        "#partitions_array.getNumPartitions() # 2\n",
        "#list2_rdd.glom().collect() # this is still having 2 only.\n",
        "list3_rdd.glom().collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2, 3, 7, 8, 9], [4, 5, 6]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1ZoRb3__UOt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4343b05e-1601-45a6-8f25-2d8d997b7932"
      },
      "source": [
        "partitions_array = list3_rdd.coalesce(2) # only decreases and it doesnot have shuffling.\n",
        "# Note: If the existing partation is x then the coalesce value cannot be more than the x. Even we pass more than the x but it does not change the number of partition.\n",
        "partitions_array.glom().collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2, 3, 7, 8, 9], [4, 5, 6]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3YRw__BgF-g"
      },
      "source": [
        "###Inferred Schemas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQBriNDSgLWY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dd38344-08af-4d8d-bccf-b1ea142bdbce"
      },
      "source": [
        "lines_rdd = sc.textFile(\"datasets/students.txt\")\n",
        "lines_rdd.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Emily,44,55,78', 'Andy,47,34,89', 'Rick,55,78,55', 'Aaron,66,34,98']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-ozpuHug5FA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d65c1dc6-1953-427d-fd92-a81a783b2281"
      },
      "source": [
        "parts_rdd = lines_rdd.map(lambda l: l.split(\",\"))\n",
        "parts_rdd.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Emily', '44', '55', '78'],\n",
              " ['Andy', '47', '34', '89'],\n",
              " ['Rick', '55', '78', '55'],\n",
              " ['Aaron', '66', '34', '98']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxxDpGA1hAgS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dee8909-8b08-46ef-d448-a093517aa84a"
      },
      "source": [
        "students_rdd = parts_rdd.map(lambda p: Row(name=p[0], math=int(p[1]), english=int(p[2]), science=int(p[3])))\n",
        "students_rdd.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(name='Emily', math=44, english=55, science=78),\n",
              " Row(name='Andy', math=47, english=34, science=89),\n",
              " Row(name='Rick', math=55, english=78, science=55),\n",
              " Row(name='Aaron', math=66, english=34, science=98)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRX6tudJhpsJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87b7e921-4243-444d-cc45-f3553f8aa1ba"
      },
      "source": [
        "students_df = spark.createDataFrame(students_rdd)\n",
        "students_df.createOrReplaceTempView(\"students_tbl\")\n",
        "students_df.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['name', 'math', 'english', 'science']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCMxRSVzibe6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14e23921-6e44-4f94-ee0f-26fe78895413"
      },
      "source": [
        "students_df.schema # Spark generates schema automatically."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType(List(StructField(name,StringType,true),StructField(math,LongType,true),StructField(english,LongType,true),StructField(science,LongType,true)))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWLKei5IifSC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "148fd6e8-4609-4b60-ebdf-8a116be2e929"
      },
      "source": [
        "spark.sql(\"SELECT * FROM students_tbl\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+----+-------+-------+\n",
            "| name|math|english|science|\n",
            "+-----+----+-------+-------+\n",
            "|Emily|  44|     55|     78|\n",
            "| Andy|  47|     34|     89|\n",
            "| Rick|  55|     78|     55|\n",
            "|Aaron|  66|     34|     98|\n",
            "+-----+----+-------+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fiaqAX4iom2"
      },
      "source": [
        " ### Explicit Schemas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uS7JFmtYivqk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "200830fb-fcb7-4f11-bdd6-48c0c6ff653e"
      },
      "source": [
        "students_rdd.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(name='Emily', math=44, english=55, science=78),\n",
              " Row(name='Andy', math=47, english=34, science=89),\n",
              " Row(name='Rick', math=55, english=78, science=55),\n",
              " Row(name='Aaron', math=66, english=34, science=98)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-40CQDRi3Br",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd09a79d-b384-4758-cfe5-659c309f91da"
      },
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
        "\n",
        "fields = [StructField('name', StringType(), True),\n",
        "          StructField('math', LongType(), True),\n",
        "          StructField('english', LongType(), True),\n",
        "          StructField('science', LongType(), True),\n",
        "]\n",
        "student_schema = StructType(fields)\n",
        "\n",
        "students_explicit_df = spark.createDataFrame(students_rdd, student_schema)\n",
        "students_explicit_df.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['name', 'math', 'english', 'science']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYrNGDDIzXix",
        "outputId": "efe755b4-42e1-41fe-92f5-022a00c169e8"
      },
      "source": [
        "students_explicit_df.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- math: long (nullable = true)\n",
            " |-- english: long (nullable = true)\n",
            " |-- science: long (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhgR7hFcjobz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40e8f58e-7a08-48c0-edc1-1b4765724721"
      },
      "source": [
        "students_explicit_df.schema"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType(List(StructField(name,StringType,true),StructField(math,LongType,true),StructField(english,LongType,true),StructField(science,LongType,true)))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOU47JWmyrvi",
        "outputId": "f16e49cc-1bcb-4fa8-ca84-eb55166dcba2"
      },
      "source": [
        "students_explicit_df.dtypes # It shows column names with its data types."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('name', 'string'),\n",
              " ('math', 'bigint'),\n",
              " ('english', 'bigint'),\n",
              " ('science', 'bigint')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDY6CX93-7TM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91f5520c-07d9-4b27-bcd4-c95566367872"
      },
      "source": [
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# explicit change the data type\n",
        "students_dt_df = students_explicit_df.withColumn('ScienceInt', students_explicit_df.math.cast('integer'))\\\n",
        "                                    .withColumn('MathInt', F.col(\"math\").cast(IntegerType()))\n",
        "students_dt_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[name: string, math: bigint, english: bigint, science: bigint, ScienceInt: int, MathInt: int]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuUpqahufB8D"
      },
      "source": [
        "### RDD Lineage(Operator/Dependency Graph)\n",
        "Graph of transformation operations required to execute when an action is called."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aq8n72wIfpfp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84dd9045-8b49-4fc8-f379-ad3cfa338341"
      },
      "source": [
        "parts_rdd.toDebugString()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'(2) PythonRDD[144] at collect at <ipython-input-39-1c3a6166eca7>:2 []\\n |  datasets/students.txt MapPartitionsRDD[143] at textFile at NativeMethodAccessorImpl.java:0 []\\n |  datasets/students.txt HadoopRDD[142] at textFile at NativeMethodAccessorImpl.java:0 []'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hRFZzYReTBN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b798bbd2-5b30-4b50-df43-1e751df1934b"
      },
      "source": [
        "parts_df = parts_rdd.toDF()\n",
        "parts_df.explain()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "== Physical Plan ==\n",
            "*(1) Scan ExistingRDD[_1#476,_2#477,_3#478,_4#479]\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}