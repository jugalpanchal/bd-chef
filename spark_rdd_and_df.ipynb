{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spark_rdd_and_df.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOsjD+iw8VwsStSPcqRxdAH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jugalpanchal/bd-chef/blob/main/spark_rdd_and_df.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5-JcY1SaVHd"
      },
      "source": [
        "# Follow the steps to install the dependencies:\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # install java\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz # spark package download\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz # unzip spark package\n",
        "!pip install -q findspark # install spark\n",
        "\n",
        "# Set the location of Java and Spark:\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUbmGLQVaeel"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import Row\n",
        "\n",
        "# create or get spark session\n",
        "spark = SparkSession.builder \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .appName(\"Spark_App1\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "# Spark 2.x does not give direct sparkcontext so we need to get it from SparkSession\n",
        "sc = spark.sparkContext\n",
        "#sc.appName #Spark_App1\n",
        "#sc.uiWebUrl # WebUI\n",
        "#sc.applicationId # application_xxxxxxxxxx_xxxx\n",
        "\n",
        "\n",
        "#sc: SparkContext\n",
        "  #1. play with RDD.\n",
        "  #2. It comes from the Spark-Core\n",
        "  #3. One only per application\n",
        "#spark: SparkSession\n",
        "  #1. play with DataFrame. \n",
        "  #2. It comes from the Spark-SQL.\n",
        "  #3. It merges SQLContext and HiveContext into one object.\n",
        "  #4. We can have multiple spark session objects in single application\n",
        "  #5. It allows us to access the SparkContext\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwtAMd19usN4"
      },
      "source": [
        "### RDD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_37d7reah0a",
        "outputId": "1df71830-13a2-4ac5-a133-11a7e5936e60"
      },
      "source": [
        "# A RDD is a collection of object of any type. It's schemaless.\n",
        "# There are three way we can create a RDD. 1. Using the parallelize method 2. Reading a file 3. From the another RDD\n",
        "# Read a file and create a RDD - RDD Started in Spark 1.x\n",
        "collection_rdd = sc.textFile(\"sample_data/anscombe.json\")\n",
        "#collection_rdd = sc.textFile(\"sample_data/*.json\") # multiple files\n",
        "\n",
        "# collection_rdd # sample_data/anscombe.json MapPartitionsRDD[260] at textFile at NativeMethodAccessorImpl.java:0\n",
        "collection_rdd.setName(\"anscombe_rdd\") # The Spark gives a internal name but we can specify the way we want. Even it helps in the SparkUI to identify specific RDD.\n",
        "collection_rdd"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "anscombe_rdd MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUEvTm7itzMO"
      },
      "source": [
        "#collection_rdd.count() #49\n",
        "#collection_rdd.take(5) # top 5 entities"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_d_B3TMMbZ3M",
        "outputId": "72762380-fe33-4784-a9d7-c574b76ce478"
      },
      "source": [
        "collection_rdd.collect() # this is like a collection"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[',\n",
              " '  {\"Series\":\"I\", \"X\":10.0, \"Y\":8.04},',\n",
              " '  {\"Series\":\"I\", \"X\":8.0, \"Y\":6.95},',\n",
              " '  {\"Series\":\"I\", \"X\":13.0, \"Y\":7.58},',\n",
              " '  {\"Series\":\"I\", \"X\":9.0, \"Y\":8.81},',\n",
              " '  {\"Series\":\"I\", \"X\":11.0, \"Y\":8.33},',\n",
              " '  {\"Series\":\"I\", \"X\":14.0, \"Y\":9.96},',\n",
              " '  {\"Series\":\"I\", \"X\":6.0, \"Y\":7.24},',\n",
              " '  {\"Series\":\"I\", \"X\":4.0, \"Y\":4.26},',\n",
              " '  {\"Series\":\"I\", \"X\":12.0, \"Y\":10.84},',\n",
              " '  {\"Series\":\"I\", \"X\":7.0, \"Y\":4.81},',\n",
              " '  {\"Series\":\"I\", \"X\":5.0, \"Y\":5.68},',\n",
              " '',\n",
              " '  {\"Series\":\"II\", \"X\":10.0, \"Y\":9.14},',\n",
              " '  {\"Series\":\"II\", \"X\":8.0, \"Y\":8.14},',\n",
              " '  {\"Series\":\"II\", \"X\":13.0, \"Y\":8.74},',\n",
              " '  {\"Series\":\"II\", \"X\":9.0, \"Y\":8.77},',\n",
              " '  {\"Series\":\"II\", \"X\":11.0, \"Y\":9.26},',\n",
              " '  {\"Series\":\"II\", \"X\":14.0, \"Y\":8.10},',\n",
              " '  {\"Series\":\"II\", \"X\":6.0, \"Y\":6.13},',\n",
              " '  {\"Series\":\"II\", \"X\":4.0, \"Y\":3.10},',\n",
              " '  {\"Series\":\"II\", \"X\":12.0, \"Y\":9.13},',\n",
              " '  {\"Series\":\"II\", \"X\":7.0, \"Y\":7.26},',\n",
              " '  {\"Series\":\"II\", \"X\":5.0, \"Y\":4.74},',\n",
              " '',\n",
              " '  {\"Series\":\"III\", \"X\":10.0, \"Y\":7.46},',\n",
              " '  {\"Series\":\"III\", \"X\":8.0, \"Y\":6.77},',\n",
              " '  {\"Series\":\"III\", \"X\":13.0, \"Y\":12.74},',\n",
              " '  {\"Series\":\"III\", \"X\":9.0, \"Y\":7.11},',\n",
              " '  {\"Series\":\"III\", \"X\":11.0, \"Y\":7.81},',\n",
              " '  {\"Series\":\"III\", \"X\":14.0, \"Y\":8.84},',\n",
              " '  {\"Series\":\"III\", \"X\":6.0, \"Y\":6.08},',\n",
              " '  {\"Series\":\"III\", \"X\":4.0, \"Y\":5.39},',\n",
              " '  {\"Series\":\"III\", \"X\":12.0, \"Y\":8.15},',\n",
              " '  {\"Series\":\"III\", \"X\":7.0, \"Y\":6.42},',\n",
              " '  {\"Series\":\"III\", \"X\":5.0, \"Y\":5.73},',\n",
              " '',\n",
              " '  {\"Series\":\"IV\", \"X\":8.0, \"Y\":6.58},',\n",
              " '  {\"Series\":\"IV\", \"X\":8.0, \"Y\":5.76},',\n",
              " '  {\"Series\":\"IV\", \"X\":8.0, \"Y\":7.71},',\n",
              " '  {\"Series\":\"IV\", \"X\":8.0, \"Y\":8.84},',\n",
              " '  {\"Series\":\"IV\", \"X\":8.0, \"Y\":8.47},',\n",
              " '  {\"Series\":\"IV\", \"X\":8.0, \"Y\":7.04},',\n",
              " '  {\"Series\":\"IV\", \"X\":8.0, \"Y\":5.25},',\n",
              " '  {\"Series\":\"IV\", \"X\":19.0, \"Y\":12.50},',\n",
              " '  {\"Series\":\"IV\", \"X\":8.0, \"Y\":5.56},',\n",
              " '  {\"Series\":\"IV\", \"X\":8.0, \"Y\":7.91},',\n",
              " '  {\"Series\":\"IV\", \"X\":8.0, \"Y\":6.89}',\n",
              " ']']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQVAnEWsuvX2"
      },
      "source": [
        "### DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsOowOUJak8W",
        "outputId": "4d06007d-4e67-4d98-8174-8d2833151e9c"
      },
      "source": [
        "# Read a file and create a DataFrame - DataFrame Started in Spark 2.x but on top of the RDD.\n",
        "json_df = spark.read.json('sample_data/anscombe.json') # first row is always a columns row.\n",
        "json_df"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Series: string, X: double, Y: double, _corrupt_record: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkNpX4Uxbpyc",
        "outputId": "4b9fbcc6-9ff1-4eeb-e41a-60579dc0bae4"
      },
      "source": [
        "json_df.printSchema()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Series: string (nullable = true)\n",
            " |-- X: double (nullable = true)\n",
            " |-- Y: double (nullable = true)\n",
            " |-- _corrupt_record: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sa0oFO4hbf1e",
        "outputId": "ab6c589e-5cad-46dd-bc72-c9526aebaefe"
      },
      "source": [
        "json_df.collect()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(Series=None, X=None, Y=None, _corrupt_record='['),\n",
              " Row(Series='I', X=10.0, Y=8.04, _corrupt_record=None),\n",
              " Row(Series='I', X=8.0, Y=6.95, _corrupt_record=None),\n",
              " Row(Series='I', X=13.0, Y=7.58, _corrupt_record=None),\n",
              " Row(Series='I', X=9.0, Y=8.81, _corrupt_record=None),\n",
              " Row(Series='I', X=11.0, Y=8.33, _corrupt_record=None),\n",
              " Row(Series='I', X=14.0, Y=9.96, _corrupt_record=None),\n",
              " Row(Series='I', X=6.0, Y=7.24, _corrupt_record=None),\n",
              " Row(Series='I', X=4.0, Y=4.26, _corrupt_record=None),\n",
              " Row(Series='I', X=12.0, Y=10.84, _corrupt_record=None),\n",
              " Row(Series='I', X=7.0, Y=4.81, _corrupt_record=None),\n",
              " Row(Series='I', X=5.0, Y=5.68, _corrupt_record=None),\n",
              " Row(Series='II', X=10.0, Y=9.14, _corrupt_record=None),\n",
              " Row(Series='II', X=8.0, Y=8.14, _corrupt_record=None),\n",
              " Row(Series='II', X=13.0, Y=8.74, _corrupt_record=None),\n",
              " Row(Series='II', X=9.0, Y=8.77, _corrupt_record=None),\n",
              " Row(Series='II', X=11.0, Y=9.26, _corrupt_record=None),\n",
              " Row(Series='II', X=14.0, Y=8.1, _corrupt_record=None),\n",
              " Row(Series='II', X=6.0, Y=6.13, _corrupt_record=None),\n",
              " Row(Series='II', X=4.0, Y=3.1, _corrupt_record=None),\n",
              " Row(Series='II', X=12.0, Y=9.13, _corrupt_record=None),\n",
              " Row(Series='II', X=7.0, Y=7.26, _corrupt_record=None),\n",
              " Row(Series='II', X=5.0, Y=4.74, _corrupt_record=None),\n",
              " Row(Series='III', X=10.0, Y=7.46, _corrupt_record=None),\n",
              " Row(Series='III', X=8.0, Y=6.77, _corrupt_record=None),\n",
              " Row(Series='III', X=13.0, Y=12.74, _corrupt_record=None),\n",
              " Row(Series='III', X=9.0, Y=7.11, _corrupt_record=None),\n",
              " Row(Series='III', X=11.0, Y=7.81, _corrupt_record=None),\n",
              " Row(Series='III', X=14.0, Y=8.84, _corrupt_record=None),\n",
              " Row(Series='III', X=6.0, Y=6.08, _corrupt_record=None),\n",
              " Row(Series='III', X=4.0, Y=5.39, _corrupt_record=None),\n",
              " Row(Series='III', X=12.0, Y=8.15, _corrupt_record=None),\n",
              " Row(Series='III', X=7.0, Y=6.42, _corrupt_record=None),\n",
              " Row(Series='III', X=5.0, Y=5.73, _corrupt_record=None),\n",
              " Row(Series='IV', X=8.0, Y=6.58, _corrupt_record=None),\n",
              " Row(Series='IV', X=8.0, Y=5.76, _corrupt_record=None),\n",
              " Row(Series='IV', X=8.0, Y=7.71, _corrupt_record=None),\n",
              " Row(Series='IV', X=8.0, Y=8.84, _corrupt_record=None),\n",
              " Row(Series='IV', X=8.0, Y=8.47, _corrupt_record=None),\n",
              " Row(Series='IV', X=8.0, Y=7.04, _corrupt_record=None),\n",
              " Row(Series='IV', X=8.0, Y=5.25, _corrupt_record=None),\n",
              " Row(Series='IV', X=19.0, Y=12.5, _corrupt_record=None),\n",
              " Row(Series='IV', X=8.0, Y=5.56, _corrupt_record=None),\n",
              " Row(Series='IV', X=8.0, Y=7.91, _corrupt_record=None),\n",
              " Row(Series='IV', X=8.0, Y=6.89, _corrupt_record=None),\n",
              " Row(Series=None, X=None, Y=None, _corrupt_record=']')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vpVtYVAu5PE"
      },
      "source": [
        "### RDD to DataFrame and vice versa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkY73YXpu8BO"
      },
      "source": [
        "generic_data = sc.parallelize([1, 'Alice', 50, 'Bob', 'Canton']) # RDD collection supports different type of items even in a nested colletion.\n",
        "#generic_data.toDF() # It does not work because it contains object with differnt types."
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezAapIlKwzMi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3491dff6-9666-4521-aad9-2e21d13e8463"
      },
      "source": [
        "people_list = [('Alice', 25), ('Bob', 20), ('Charly', 30), ('Don', 15)]\n",
        "people_rdd_pair = sc.parallelize(people_list) # create a RDD but with Tuple/Pair objects\n",
        "#people_rdd_pair # ParallelCollectionRDD[2] at readRDDFromFile\n",
        "\n",
        "people_rdd_pair.collect()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Alice', 25), ('Bob', 20), ('Charly', 30), ('Don', 15)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G26PuH0xzoj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7173109c-763d-4a31-90c7-105e4df218f1"
      },
      "source": [
        "people_df = people_rdd_pair.toDF() # Column names have been automatically generated and assigned. It infers data types too.\n",
        "#people_df # DataFrame[_1: string, _2: bigint]\n",
        "people_df.collect() # shows data with its class"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(_1='Alice', _2=25),\n",
              " Row(_1='Bob', _2=20),\n",
              " Row(_1='Charly', _2=30),\n",
              " Row(_1='Don', _2=15)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyTXqJ63zQmI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37c2a858-1c8a-48cc-c444-0afee727ff7d"
      },
      "source": [
        "people_df.show() # shows in tabular format"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+---+\n",
            "|    _1| _2|\n",
            "+------+---+\n",
            "| Alice| 25|\n",
            "|   Bob| 20|\n",
            "|Charly| 30|\n",
            "|   Don| 15|\n",
            "+------+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrXYm9SLyXup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df3fc6ee-aafc-447b-b70d-e4a08d7d0370"
      },
      "source": [
        "from pyspark import Row\n",
        "# There are two way to get custom column names.\n",
        "# 1. Create the row class object while creating a RDD\n",
        "data_rdd_row = sc.parallelize([Row(name='Alice', age=50), \n",
        "                               Row(name='Bob', age=20)])\n",
        "data_rdd_row.collect() # [Row(name='Alice', age=50), Row(name='Bob', age=20)]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(name='Alice', age=50), Row(name='Bob', age=20)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4E_xlfUK1fDn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0948edc5-1940-4548-93d6-387ce199d009"
      },
      "source": [
        "# 2. Transform existing RDD to new RDD\n",
        "people_rdd_row = people_rdd_pair.map(lambda x: Row(name=x[0], age=int(x[1]))) # create a RDD with Row objects\n",
        "people_rdd_row # PythonRDD[63] at RDD\n",
        "people_rdd_row.collect()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(name='Alice', age=25),\n",
              " Row(name='Bob', age=20),\n",
              " Row(name='Charly', age=30),\n",
              " Row(name='Don', age=15)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZpnYejtx8Ly",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48d49a47-a21d-41e6-d4ab-79250de965dd"
      },
      "source": [
        "data_rdd_row.toDF().show()\n",
        "people_rdd_row.toDF().show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+---+\n",
            "| name|age|\n",
            "+-----+---+\n",
            "|Alice| 50|\n",
            "|  Bob| 20|\n",
            "+-----+---+\n",
            "\n",
            "+------+---+\n",
            "|  name|age|\n",
            "+------+---+\n",
            "| Alice| 25|\n",
            "|   Bob| 20|\n",
            "|Charly| 30|\n",
            "|   Don| 15|\n",
            "+------+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXbgAOjI3hdv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "296d67f0-e08c-44bc-d121-7cc524f237bd"
      },
      "source": [
        "people_df.rdd # Every DataFrame contains a RDD and we can get the dataframe's RDD easily."
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MapPartitionsRDD[39] at javaToPython at NativeMethodAccessorImpl.java:0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKBZpCu8-lrV"
      },
      "source": [
        "people_df = data_rdd_row.toDF()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uf63F5uN2NEB"
      },
      "source": [
        "### Transformation on DataFrame\n",
        "####FlatMap, Map, ReduceByKey"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hymbpY6m2hh1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a50da72-617e-4c62-e609-b438e03105dc"
      },
      "source": [
        "# combining - within partition\n",
        "# merging - across partition\n",
        "\n",
        "# map operation on RDD\n",
        "people_rdd = people_df.rdd.map(lambda x: (x.age, x.age + 5))\n",
        "#people_rdd # PythonRDD[78] at RDD\n",
        "people_rdd.collect()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(50, 55), (20, 25)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B07N6Uvi5N6B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c78062fb-2527-4798-a97a-5a85fc61cc3c"
      },
      "source": [
        "# Map: the map operation performs a transformation on every element in the RDD. \n",
        "# Similar kind of method is not available for the DataFrame, \n",
        "# so we compute and assign computed value in new column within the DataFrame. \n",
        "\n",
        "people_df_new_age = people_df.select('name', 'age').withColumn('new_age', people_df.age + 10) # we can type people_df.age to mention column name.\n",
        "people_df_new_age.show()\n",
        "\n",
        "people_df_new_age = people_df_new_age.withColumnRenamed('new_age', 'after_ten_years')\n",
        "people_df_new_age = people_df_new_age.drop('age')\n",
        "people_df_new_age.show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+---+-------+\n",
            "| name|age|new_age|\n",
            "+-----+---+-------+\n",
            "|Alice| 50|     60|\n",
            "|  Bob| 20|     30|\n",
            "+-----+---+-------+\n",
            "\n",
            "+-----+---------------+\n",
            "| name|after_ten_years|\n",
            "+-----+---------------+\n",
            "|Alice|             60|\n",
            "|  Bob|             30|\n",
            "+-----+---------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEfggXez5GNz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72b5c263-2092-4d1f-fe67-a1158ac7432e"
      },
      "source": [
        "people_df.first()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(name='Alice', age=50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecH5aObO2hqE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74e8bf39-f4fe-423c-fa9d-4cca579a26a5"
      },
      "source": [
        "people_df.take(2)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(name='Alice', age=50), Row(name='Bob', age=20)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61qXp_Ja2uZM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0c8c5cba-7b2b-4458-c4a8-7991e3499d24"
      },
      "source": [
        "people_df.collect()[1][0] # It is a tabular format so can access specific cell.\n",
        "# It always creates a clone then return so if the cell has any class type and we modify then it won't change in the dataframe."
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Bob'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885
        },
        "id": "s4W9Y2c3_Maq",
        "outputId": "f7247b64-7db7-443e-a056-baa802d343ce"
      },
      "source": [
        "\"\"\"\n",
        "Text in the datasets/simple_titles.txt:\n",
        "How can I use DataFrames in 2.0\n",
        "What is an RDD and Schema RDD\n",
        "How do I group by a field\n",
        "Can I use Hive from HUE\n",
        "\"\"\"\n",
        "\n",
        "# read lines\n",
        "lines_rdd = sc.textFile(\"datasets/simple_titles.txt\")\n",
        "print('lines', lines_rdd.collect())\n",
        "\n",
        "# flat all words\n",
        "words_rdd = lines_rdd.flatMap(lambda line: line.split(' '))\n",
        "print('words', words_rdd.collect())\n",
        "\n",
        "# map each word with 1 value\n",
        "word_and_count_rdd = words_rdd.map(lambda x: (x,1))\n",
        "print('word_for_count', word_and_count_rdd.collect())\n",
        "\n",
        "# count each word by key\n",
        "word_with_count_rdd = word_and_count_rdd.reduceByKey(lambda x,y: x + y)\n",
        "print('word_with_count_rdd', word_with_count_rdd.collect())"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-66f43fd0efa7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# read lines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mlines_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"datasets/simple_titles.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lines'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# flat all words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \"\"\"\n\u001b[1;32m    948\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/content/datasets/simple_titles.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:297)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:239)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:325)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor58.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_-2qtG4g6x7"
      },
      "source": [
        "dir(people_df) # check all the methods which can be used with the RDD."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_jOgc3Bi5ez"
      },
      "source": [
        "#help(people_df) # even better with example but it gives details about all the methods.\n",
        "help(people_df.collect) # for specific method"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_vZ3WzLdKdV"
      },
      "source": [
        "###Partition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQtZZ1uNdMYy"
      },
      "source": [
        "list1_rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
        "#list1_rdd # ParallelCollectionRDD[1] at readRDDFromFile at PythonRDD.scala:274\n",
        "#list1_rdd.collect() # [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "#list1_rdd.getNumPartitions() # 2 - even though it is small data it has created 2 partition.\n",
        "partitions_array = list1_rdd.glom() # gluam method bring all the partition as individual partition. don't try with big valume data.\n",
        "partitions_array.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHtdeIFufMYa"
      },
      "source": [
        "list2_rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9], 3) # specify partition explicitly\n",
        "list2_rdd.getNumPartitions() # 3\n",
        "partitions_array = list2_rdd.glom()\n",
        "partitions_array.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUebOIFU-hFl"
      },
      "source": [
        "list3_rdd = list2_rdd.repartition(2) # specifies new partition, up or down but there would be shuffling.\n",
        "#partitions_array.getNumPartitions() # 2\n",
        "#list2_rdd.glom().collect() # this is still having 2 only.\n",
        "list3_rdd.glom().collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1ZoRb3__UOt"
      },
      "source": [
        "partitions_array = list3_rdd.coalesce(2) # only decreases and it doesnot have shuffling.\n",
        "# Note: If the existing partation is x then the coalesce value cannot be more than the x. Even we pass more than the x but it does not change the number of partition.\n",
        "partitions_array.glom().collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3YRw__BgF-g"
      },
      "source": [
        "###Inferred Schemas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQBriNDSgLWY"
      },
      "source": [
        "lines_rdd = sc.textFile(\"datasets/students.txt\")\n",
        "lines_rdd.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-ozpuHug5FA"
      },
      "source": [
        "parts_rdd = lines_rdd.map(lambda l: l.split(\",\"))\n",
        "parts_rdd.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxxDpGA1hAgS"
      },
      "source": [
        "students_rdd = parts_rdd.map(lambda p: Row(name=p[0], math=int(p[1]), english=int(p[2]), science=int(p[3])))\n",
        "students_rdd.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRX6tudJhpsJ"
      },
      "source": [
        "students_df = spark.createDataFrame(students_rdd)\n",
        "students_df.createOrReplaceTempView(\"students_tbl\")\n",
        "students_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCMxRSVzibe6"
      },
      "source": [
        "students_df.schema # Spark generates schema automatically."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWLKei5IifSC"
      },
      "source": [
        "spark.sql(\"SELECT * FROM students_tbl\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fiaqAX4iom2"
      },
      "source": [
        " ### Explicit Schemas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uS7JFmtYivqk"
      },
      "source": [
        "parts_rdd.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-40CQDRi3Br"
      },
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
        "\n",
        "fields = [StructField('name', StringType(), True),\n",
        "          StructField('math', LongType(), True),\n",
        "          StructField('english', LongType(), True),\n",
        "          StructField('science', LongType(), True),\n",
        "]\n",
        "student_schema = StructType(fields)\n",
        "\n",
        "students_explicit_df = spark.createDataFrame(parts_rdd, student_schema)\n",
        "students_explicit_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhgR7hFcjobz"
      },
      "source": [
        "students_explicit_df.schema"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuUpqahufB8D"
      },
      "source": [
        "### RDD Lineage(Operator/Dependency Graph)\n",
        "Graph of transformation operations required to execute when an action is called."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aq8n72wIfpfp"
      },
      "source": [
        "parts_rdd.toDebugString()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}