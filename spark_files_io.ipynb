{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spark_files_io.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOmml67yYM02Ohf7u3PMMGE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jugalpanchal/bd-chef/blob/main/spark_files_io.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1mj2GmZXFj7"
      },
      "source": [
        "# Follow the steps to install the dependencies:\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # install java\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz # spark package download\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz # unzip spark package\n",
        "!pip install -q findspark # install spark\n",
        "\n",
        "# Set the location of Java and Spark:\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yg3KVcA3XIw8"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# create or get spark session\n",
        "spark = SparkSession.builder \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .appName(\"Spark_App1\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAX2Ab6CXhY8"
      },
      "source": [
        "If the csv file has a header (column names in the first row) then set header=true. This will use the first row in the csv file as the dataframe's column names. Setting header=false (default option) will result in a dataframe with default column names: _c0, _c1, _c2, etc.\n",
        "If the csv file has a header row and we do not specifiy header=true then the header row would be consider as a data row."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfsZQTTuXiMZ",
        "outputId": "f7a76b9b-252e-4436-df78-11af7ac5aca2"
      },
      "source": [
        "data = spark.read.csv('sample_data/california_housing_test.csv')\n",
        "\n",
        "data \n",
        "# DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string]\n",
        "\n",
        "#data.collect()\n",
        "#Row(_c0='longitude', _c1='latitude', _c2='housing_median_age', _c3='total_rooms', _c4='total_bedrooms', _c5='population', _c6='households', _c7='median_income', _c8='median_house_value'),\n",
        "# Row(_c0='-122.050000', _c1='37.370000', _c2='27.000000', _c3='3885.000000', _c4='661.000000', _c5='1537.000000', _c6='606.000000', _c7='6.608500', _c8='344700.000000'),\n",
        "# Row(_c0='-118.300000', _c1='34.260000', _c2='43.000000', _c3='1510.000000', _c4='310.000000', _c5='809.000000', _c6='277.000000', _c7='3.599000', _c8='176500.000000'),\n",
        "# Row(_c0='-117.810000', _c1='33.780000', _c2='27.000000', _c3='3589.000000', _c4='507.000000', _c5='1484.000000', _c6='495.000000', _c7='5.793400', _c8='270500.000000')\n",
        "# ...]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvfjsnEMXkJB",
        "outputId": "c9b96f02-85a8-419c-e4d1-13d67e388b84"
      },
      "source": [
        "data = spark.read.csv('sample_data/california_housing_test.csv', header=True)\n",
        "\n",
        "data\n",
        "# DataFrame[longitude: string, latitude: string, housing_median_age: string, total_rooms: string, total_bedrooms: string, population: string, households: string, median_income: string, median_house_value: string]\n",
        "\n",
        "#data.collect()\n",
        "#[Row(longitude='-122.050000', latitude='37.370000', housing_median_age='27.000000', total_rooms='3885.000000', total_bedrooms='661.000000', population='1537.000000', households='606.000000', median_income='6.608500', median_house_value='344700.000000'),\n",
        "# Row(longitude='-118.300000', latitude='34.260000', housing_median_age='43.000000', total_rooms='1510.000000', total_bedrooms='310.000000', population='809.000000', households='277.000000', median_income='3.599000', median_house_value='176500.000000'),\n",
        "# Row(longitude='-117.810000', latitude='33.780000', housing_median_age='27.000000', total_rooms='3589.000000', total_bedrooms='507.000000', population='1484.000000', households='495.000000', median_income='5.793400', median_house_value='270500.000000'),\n",
        "# ...]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[longitude: string, latitude: string, housing_median_age: string, total_rooms: string, total_bedrooms: string, population: string, households: string, median_income: string, median_house_value: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGYYdzJ_Xoim"
      },
      "source": [
        "inferSchema=True let the Spark decides the schema type depend on values in the file. It infers the schema of each column.\n",
        "inferSchema=False(default option) it makes every columns as string type.\n",
        "\n",
        "The infer is a costly operation so we can provide schema manually. Even it helps sometime if we do not have a header row.\n",
        "https://stackoverflow.com/questions/39926411/provide-schema-while-reading-csv-file-as-a-dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fGp6YKAXpF-",
        "outputId": "6a992d9b-b603-4561-a8f6-0a4f7b95504d"
      },
      "source": [
        "data = spark.read.csv('sample_data/california_housing_test.csv', header=True, inferSchema=True)\n",
        "\n",
        "data\n",
        "# DataFrame[longitude: double, latitude: double, housing_median_age: double, total_rooms: double, total_bedrooms: double, population: double, households: double, median_income: double, median_house_value: double]\n",
        "\n",
        "#data.collect()\n",
        "#[Row(longitude=-122.05, latitude=37.37, housing_median_age=27.0, total_rooms=3885.0, total_bedrooms=661.0, population=1537.0, households=606.0, median_income=6.6085, median_house_value=344700.0),\n",
        "# Row(longitude=-118.3, latitude=34.26, housing_median_age=43.0, total_rooms=1510.0, total_bedrooms=310.0, population=809.0, households=277.0, median_income=3.599, median_house_value=176500.0),\n",
        "# Row(longitude=-117.81, latitude=33.78, housing_median_age=27.0, total_rooms=3589.0, total_bedrooms=507.0, population=1484.0, households=495.0, median_income=5.7934, median_house_value=270500.0),\n",
        "#...]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[longitude: double, latitude: double, housing_median_age: double, total_rooms: double, total_bedrooms: double, population: double, households: double, median_income: double, median_house_value: double]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mkxwimqXteh",
        "outputId": "2ce9d871-9dd5-4fcb-86af-4a0880081868"
      },
      "source": [
        "data.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- longitude: double (nullable = true)\n",
            " |-- latitude: double (nullable = true)\n",
            " |-- housing_median_age: double (nullable = true)\n",
            " |-- total_rooms: double (nullable = true)\n",
            " |-- total_bedrooms: double (nullable = true)\n",
            " |-- population: double (nullable = true)\n",
            " |-- households: double (nullable = true)\n",
            " |-- median_income: double (nullable = true)\n",
            " |-- median_house_value: double (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZpPw21sXwHh",
        "outputId": "338d2df5-d44c-4b20-cfc0-8d8cc5de4625"
      },
      "source": [
        "filtered_data = data.select('longitude', 'latitude', 'median_house_value')\n",
        "filtered_data # DataFrame[longitude: double, latitude: double, median_house_value: double]\n",
        "\n",
        "#filtered_data.collect()\n",
        "#[Row(longitude=-122.05, latitude=37.37, median_house_value=344700.0),\n",
        "# Row(longitude=-118.3, latitude=34.26, median_house_value=176500.0),\n",
        "# Row(longitude=-117.81, latitude=33.78, median_house_value=270500.0),\n",
        "#...]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[longitude: double, latitude: double, median_house_value: double]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kYUpdrOVgmi"
      },
      "source": [
        "### Save - write"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-b-8Rk4XyBw"
      },
      "source": [
        "# It creates number of files equal to the partitions of the dataframe.\n",
        "# It creates a folder and will have part-00000-guid.csv files and _SUCCESS maker.\n",
        "# Without header\n",
        "filtered_data.write.csv(\"sample_data/california_housing_filtered_dataset_csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcW6lu9_sWAX"
      },
      "source": [
        " # It creates a folder and will have a part-00000-guid.csv file and _SUCCESS maker.\n",
        " # With header\n",
        " # coalesce: combine partitions in a single file by repartition\n",
        "filtered_data.coalesce(1).write.option('header', 'true')\\\n",
        "  .csv(\"sample_data/california_housing_filtered_single_csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wI9BuchwrbB"
      },
      "source": [
        "# It creates number of files equal to the partitions of the dataframe.\n",
        "# It creates a folder and will have part-00000-guid.json files and _SUCCESS maker.\n",
        "# But it does not create json records in a array and even we do not need it because MapReduce and Spark need each record as a single entry while reading.\n",
        "filtered_data.write.json(\"sample_data/california_housing_filtered_json\")\n",
        "\n",
        "# Note: we can pass mode='PERMISSIVE' while reading any JSON file so it will replace any correpted json record with the null and add new column '_corrupt_record'.\n",
        "# Note: mode='DROPMALFORMED' it will drop the corrupted records while reading any JSON file."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdkxHMhHSWz9"
      },
      "source": [
        "# This will repartition the data and writes the data in 10 files.\n",
        "# There can be 10 executors which are writting the files so the writing process can be faster.\n",
        "filtered_data.repartition(10).write.json(\"sample_data/california_housing_filtered_json_repartition\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVwWOIoZY5kb"
      },
      "source": [
        "# In case of s3, there is a library which we need to install it then it supports 's3a://....' file path and downloads from the S3 path.\n",
        "#file = sc.textFile('s3a://bucket1/file1.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLFzAiLUVmFs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciMdwB7fajYi"
      },
      "source": [
        "###Serializing(Pickling) and Deserializing(Unpicking)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P83b3eSqY5r1"
      },
      "source": [
        "# Pickle saves in its own format so it is not a human-readable format.\n",
        "sample1_rdd = sc.textFile(\"sample_data/california_housing_test.csv\")\n",
        "sample1_rdd.saveAsPickleFile(\"sample_data/cal_hus_test_pickle\") # it decides repartitioning its own.\n",
        "\n",
        "sample2_rdd = sc.pickleFile('sample_data/cal_hus_test_pickle')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrXpE-AXA3lo"
      },
      "source": [
        "###Cache and Persist"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKvaRXcEA3xm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "2840dcad-7699-4cb0-dce6-1aab8587decf"
      },
      "source": [
        "# Spark may perform the caching of intermediate results for the expensive operations to avoid recomputation when nodes fail.\n",
        "# Cache: It caches in the memory only.\n",
        "# Persist: It persists in the memory and disk. We can specify through a parameter. If we don't need the RDD anymore then call the unpersist.\n",
        "\n",
        "#sample2_rdd.persist(MEMORY_ONLY)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-4cf3baec87aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Persist: It persists in the memory and disk. We can specify through a parameter.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msample2_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMEMORY_ONLY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'MEMORY_ONLY' is not defined"
          ]
        }
      ]
    }
  ]
}