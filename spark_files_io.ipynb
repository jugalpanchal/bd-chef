{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spark_files_io.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPL+JfuYWY68NUm8NGMfX6G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jugalpanchal/bd-chef/blob/main/spark_files_io.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1mj2GmZXFj7"
      },
      "source": [
        "# Follow the steps to install the dependencies:\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # install java\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz # spark package download\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz # unzip spark package\n",
        "!pip install -q findspark # install spark\n",
        "\n",
        "# Set the location of Java and Spark:\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yg3KVcA3XIw8"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# create or get spark session\n",
        "spark = SparkSession.builder \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .appName(\"Spark_App1\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAX2Ab6CXhY8"
      },
      "source": [
        "If the csv file has a header (column names in the first row) then set header=true. This will use the first row in the csv file as the dataframe's column names. Setting header=false (default option) will result in a dataframe with default column names: _c0, _c1, _c2, etc.\n",
        "If the csv file has a header row and we do not specifiy header=true then the header row would be consider as a data row."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfsZQTTuXiMZ",
        "outputId": "4866329c-92af-42b7-efbb-f60e93c85e4b"
      },
      "source": [
        "data = spark.read.csv('sample_data/california_housing_test.csv')\n",
        "\n",
        "data \n",
        "# DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string]\n",
        "\n",
        "#data.collect()\n",
        "#Row(_c0='longitude', _c1='latitude', _c2='housing_median_age', _c3='total_rooms', _c4='total_bedrooms', _c5='population', _c6='households', _c7='median_income', _c8='median_house_value'),\n",
        "# Row(_c0='-122.050000', _c1='37.370000', _c2='27.000000', _c3='3885.000000', _c4='661.000000', _c5='1537.000000', _c6='606.000000', _c7='6.608500', _c8='344700.000000'),\n",
        "# Row(_c0='-118.300000', _c1='34.260000', _c2='43.000000', _c3='1510.000000', _c4='310.000000', _c5='809.000000', _c6='277.000000', _c7='3.599000', _c8='176500.000000'),\n",
        "# Row(_c0='-117.810000', _c1='33.780000', _c2='27.000000', _c3='3589.000000', _c4='507.000000', _c5='1484.000000', _c6='495.000000', _c7='5.793400', _c8='270500.000000')\n",
        "# ...]\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvfjsnEMXkJB",
        "outputId": "b922bc3d-dd41-4854-cdec-3209e2299dff"
      },
      "source": [
        "data = spark.read.csv('sample_data/california_housing_test.csv', header=True)\n",
        "\n",
        "data\n",
        "# DataFrame[longitude: string, latitude: string, housing_median_age: string, total_rooms: string, total_bedrooms: string, population: string, households: string, median_income: string, median_house_value: string]\n",
        "\n",
        "#data.collect()\n",
        "#[Row(longitude='-122.050000', latitude='37.370000', housing_median_age='27.000000', total_rooms='3885.000000', total_bedrooms='661.000000', population='1537.000000', households='606.000000', median_income='6.608500', median_house_value='344700.000000'),\n",
        "# Row(longitude='-118.300000', latitude='34.260000', housing_median_age='43.000000', total_rooms='1510.000000', total_bedrooms='310.000000', population='809.000000', households='277.000000', median_income='3.599000', median_house_value='176500.000000'),\n",
        "# Row(longitude='-117.810000', latitude='33.780000', housing_median_age='27.000000', total_rooms='3589.000000', total_bedrooms='507.000000', population='1484.000000', households='495.000000', median_income='5.793400', median_house_value='270500.000000'),\n",
        "# ...]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[longitude: string, latitude: string, housing_median_age: string, total_rooms: string, total_bedrooms: string, population: string, households: string, median_income: string, median_house_value: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGYYdzJ_Xoim"
      },
      "source": [
        "inferSchema=True let the Spark decides the schema type depend on values in the file. It infers the schema of each column.\n",
        "inferSchema=False(default option) it makes every columns as string type.\n",
        "\n",
        "The infer is a costly operation so we can provide schema manually. Even it helps sometime if we do not have a header row.\n",
        "https://stackoverflow.com/questions/39926411/provide-schema-while-reading-csv-file-as-a-dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fGp6YKAXpF-",
        "outputId": "8b6773d2-968b-42e8-f7fa-c58cbd82ed8b"
      },
      "source": [
        "data = spark.read.csv('sample_data/california_housing_test.csv', header=True, inferSchema=True)\n",
        "\n",
        "data\n",
        "# DataFrame[longitude: double, latitude: double, housing_median_age: double, total_rooms: double, total_bedrooms: double, population: double, households: double, median_income: double, median_house_value: double]\n",
        "\n",
        "#data.collect()\n",
        "#[Row(longitude=-122.05, latitude=37.37, housing_median_age=27.0, total_rooms=3885.0, total_bedrooms=661.0, population=1537.0, households=606.0, median_income=6.6085, median_house_value=344700.0),\n",
        "# Row(longitude=-118.3, latitude=34.26, housing_median_age=43.0, total_rooms=1510.0, total_bedrooms=310.0, population=809.0, households=277.0, median_income=3.599, median_house_value=176500.0),\n",
        "# Row(longitude=-117.81, latitude=33.78, housing_median_age=27.0, total_rooms=3589.0, total_bedrooms=507.0, population=1484.0, households=495.0, median_income=5.7934, median_house_value=270500.0),\n",
        "#...]\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[longitude: double, latitude: double, housing_median_age: double, total_rooms: double, total_bedrooms: double, population: double, households: double, median_income: double, median_house_value: double]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mkxwimqXteh",
        "outputId": "ba2d406d-5898-480f-c60c-9118019165ec"
      },
      "source": [
        "data.printSchema()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- longitude: double (nullable = true)\n",
            " |-- latitude: double (nullable = true)\n",
            " |-- housing_median_age: double (nullable = true)\n",
            " |-- total_rooms: double (nullable = true)\n",
            " |-- total_bedrooms: double (nullable = true)\n",
            " |-- population: double (nullable = true)\n",
            " |-- households: double (nullable = true)\n",
            " |-- median_income: double (nullable = true)\n",
            " |-- median_house_value: double (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZpPw21sXwHh",
        "outputId": "b845b9e5-6c3e-4537-cfad-e8e91c767139"
      },
      "source": [
        "filtered_data = data.select('longitude', 'latitude', 'median_house_value')\n",
        "filtered_data # DataFrame[longitude: double, latitude: double, median_house_value: double]\n",
        "\n",
        "#filtered_data.collect()\n",
        "#[Row(longitude=-122.05, latitude=37.37, median_house_value=344700.0),\n",
        "# Row(longitude=-118.3, latitude=34.26, median_house_value=176500.0),\n",
        "# Row(longitude=-117.81, latitude=33.78, median_house_value=270500.0),\n",
        "#...]\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[longitude: double, latitude: double, median_house_value: double]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-b-8Rk4XyBw"
      },
      "source": [
        "# It creates number of files equal to the partitions of the dataframe.\n",
        "# It creates a folder and will have part-00000-guid.csv files and _SUCCESS maker.\n",
        "# Without header\n",
        "filtered_data.write.csv(\"sample_data/california_housing_filtered_dataset_csv\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcW6lu9_sWAX"
      },
      "source": [
        " # It creates a folder and will have a part-00000-guid.csv file and _SUCCESS maker.\n",
        " # With header\n",
        " # coalesce: combine partitions in a single file by repartition\n",
        "filtered_data.coalesce(1).write.option('header', 'true')\\\n",
        "  .csv(\"sample_data/california_housing_filtered_single_csv\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wI9BuchwrbB"
      },
      "source": [
        "# It creates number of files equal to the partitions of the dataframe.\n",
        "# It creates a folder and will have part-00000-guid.json files and _SUCCESS maker.\n",
        "# But it does not create json records in a array and even we do not need it because MapReduce and Spark need each record as a single entry while reading.\n",
        "filtered_data.write.json(\"sample_data/california_housing_filtered_json\")"
      ],
      "execution_count": 10,
      "outputs": []
    }
  ]
}