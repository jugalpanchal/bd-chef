{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spark_hive_ops.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMg/E3MS3Tcz3o094x0P/dE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jugalpanchal/bd-chef/blob/main/spark_hive_ops.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l71kit82f_ac"
      },
      "source": [
        "# Follow the steps to install the dependencies:\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # install java\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz # spark package download\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz # unzip spark package\n",
        "!pip install -q findspark # install spark\n",
        "\n",
        "# Set the location of Java and Spark:\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANgvtS-DgCtw"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# create or get spark session\n",
        "# If we don't specifiy the Hive support then it gives an error while creating a table:\n",
        "# AnalysisException: Hive support is required to CREATE Hive TABLE (AS SELECT);\n",
        "spark = SparkSession.builder \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .appName(\"Spark_App1\") \\\n",
        "        .enableHiveSupport() \\\n",
        "        .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Q9zUndRgGXd"
      },
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql import HiveContext\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "hc = HiveContext(sc) # create a Hive context\n",
        "sqc = SQLContext(sc) # create a SQL context\n",
        "\n",
        "# So does diff between these two? - \n",
        "# HiveContext is still the superset of SQLContext.\n",
        "# It contains certain extra properties such as it can read the configuration from hive-site.xml, in case you have hive use cases otherwise simply use SQLContext\n",
        "\n",
        "# The sql is an alternative of the DataFrame DSL(Domain Specfic Language ex Map, GroupBy, FlatMap etc.)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K13Th5iWtjMG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2398edc0-0949-4389-d251-72ec81999f77"
      },
      "source": [
        "hc"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.context.HiveContext at 0x7fc9e4c6f810>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-SeMAcStlw0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96dc5f7a-3c89-4d35-bc6b-31e8aae7e377"
      },
      "source": [
        "sqc"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.context.SQLContext at 0x7fc9e4c6f910>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaubDxXwmWnO"
      },
      "source": [
        "### Load Constant Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ev9O57Q7garT",
        "outputId": "1b826c81-73b6-4c67-f750-b342b8e54abc"
      },
      "source": [
        "people_list = [('A', 25), ('B', 20), ('C', 30), ('D', 15)]\n",
        "people_rdd_pair = sc.parallelize(people_list) # create a RDD but with Tuple/Pair objects\n",
        "#people_rdd_pair # ParallelCollectionRDD[2] at readRDDFromFile\n",
        "\n",
        "people_rdd_row = people_rdd_pair.map(lambda x: Row(name=x[0], age=int(x[1]))) # create a RDD with Row objects\n",
        "#people # PythonRDD[5] at RDD\n",
        "\n",
        "people_rdd_row.collect()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(name='A', age=25),\n",
              " Row(name='B', age=20),\n",
              " Row(name='C', age=30),\n",
              " Row(name='D', age=15)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MNk8Cg1jJKS",
        "outputId": "f29bda6e-0f75-4534-f4b9-2a940d94957f"
      },
      "source": [
        "people_df = spark.createDataFrame(people_rdd_row)\n",
        "#people_df # DataFrame[name: string, age: bigint]\n",
        "\n",
        "people_df.collect()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(name='A', age=25),\n",
              " Row(name='B', age=20),\n",
              " Row(name='C', age=30),\n",
              " Row(name='D', age=15)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOJS3tpis6SB"
      },
      "source": [
        "### HiveContext"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8SdL9u1j2wr",
        "outputId": "c5c9a3f5-126b-4d4a-a622-076d7b6e8b58"
      },
      "source": [
        "hc.registerDataFrameAsTable(people_df, 'people_tbl') # create a table\n",
        "hc.sql('show tables').show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+----------+-----------+\n",
            "|database| tableName|isTemporary|\n",
            "+--------+----------+-----------+\n",
            "| default| temp_tbl1|      false|\n",
            "| default| temp_tbl2|      false|\n",
            "| default| temp_tbl4|      false|\n",
            "|        |people_tbl|       true|\n",
            "+--------+----------+-----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIFqPgBRljvy",
        "outputId": "b4de665c-5989-4225-cad7-04d748783cbf"
      },
      "source": [
        "hc.sql('select * from people_tbl').show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+---+\n",
            "|name|age|\n",
            "+----+---+\n",
            "|   A| 25|\n",
            "|   B| 20|\n",
            "|   C| 30|\n",
            "|   D| 15|\n",
            "+----+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vB6YVD5mO5d"
      },
      "source": [
        "###Load JSON Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1V39qcM4msDc",
        "outputId": "13b60b7d-93ba-4cb0-97a5-b7e379aaf5bb"
      },
      "source": [
        "people_json_df = spark.read.json('sample_data/anscombe.json') # Series is not Pandas class here :) It is a key in JSON file.\n",
        "people_json_df"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Series: string, X: double, Y: double, _corrupt_record: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkR900tZnnqj",
        "outputId": "34d22f58-692b-4bee-baf3-d5865fc32896"
      },
      "source": [
        "people_json_df.registerTempTable('people_json_tbl')\n",
        "hc.sql('show tables').show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+---------------+-----------+\n",
            "|database|      tableName|isTemporary|\n",
            "+--------+---------------+-----------+\n",
            "| default|      temp_tbl1|      false|\n",
            "| default|      temp_tbl2|      false|\n",
            "| default|      temp_tbl4|      false|\n",
            "|        |people_json_tbl|       true|\n",
            "|        |     people_tbl|       true|\n",
            "+--------+---------------+-----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dG67LtSLn0gu",
        "outputId": "af76409d-6e0a-4a7b-acd5-88550bdcb842"
      },
      "source": [
        "results = hc.sql('select * from people_json_tbl')\n",
        "results.show() # default shows top 20 rows"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+----+-----+---------------+\n",
            "|Series|   X|    Y|_corrupt_record|\n",
            "+------+----+-----+---------------+\n",
            "|  null|null| null|              [|\n",
            "|     I|10.0| 8.04|           null|\n",
            "|     I| 8.0| 6.95|           null|\n",
            "|     I|13.0| 7.58|           null|\n",
            "|     I| 9.0| 8.81|           null|\n",
            "|     I|11.0| 8.33|           null|\n",
            "|     I|14.0| 9.96|           null|\n",
            "|     I| 6.0| 7.24|           null|\n",
            "|     I| 4.0| 4.26|           null|\n",
            "|     I|12.0|10.84|           null|\n",
            "|     I| 7.0| 4.81|           null|\n",
            "|     I| 5.0| 5.68|           null|\n",
            "|    II|10.0| 9.14|           null|\n",
            "|    II| 8.0| 8.14|           null|\n",
            "|    II|13.0| 8.74|           null|\n",
            "|    II| 9.0| 8.77|           null|\n",
            "|    II|11.0| 9.26|           null|\n",
            "|    II|14.0|  8.1|           null|\n",
            "|    II| 6.0| 6.13|           null|\n",
            "|    II| 4.0|  3.1|           null|\n",
            "+------+----+-----+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7tnd8OrsnE2"
      },
      "source": [
        "### If we need to connect Hadoop Yarn Hive then make sure the script is submited in the Yarn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tolKXsfxo-PJ"
      },
      "source": [
        "# Set master as yarn in that case and enable hive support\n",
        "spark = SparkSession.builder \\\n",
        "        .master(\"yarn\") \\\n",
        "        .appName(\"Spark_App1\") \\\n",
        "        .enableHiveSupport() \\\n",
        "        .getOrCreate()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw1UU5aBsq3T"
      },
      "source": [
        "### SQLContext"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tI8dlwJctL8S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf4eb859-5eba-4ec5-c177-2078b30be527"
      },
      "source": [
        "# It's a alternative way to play with data. SQLContext helps to create a DataFrame too.\n",
        "\n",
        "df = sqc.range(5)\n",
        "#df # DataFrame[id: bigint]\n",
        "df.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  0|\n",
            "|  1|\n",
            "|  2|\n",
            "|  3|\n",
            "|  4|\n",
            "+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRtO_gffkKal"
      },
      "source": [
        "### Partition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1N1u3YqCqFKo",
        "outputId": "1e034487-2b8a-42e5-fc08-a7a59338db71"
      },
      "source": [
        "# 1. It won't delete the files and folders. because it is an external table.\n",
        "# Table or view 'temp_tbl1' already exists in database 'default'\n",
        "spark.sql('drop table if exists temp_tbl1')\n",
        "# 2. It creates a spark-warehouse/ext/temp_db/temp_tbl2/datekey=2021-06-20/_SUCCESS\n",
        "# CREATE EXTERNAL TABLE must be accompanied by LOCATION\n",
        "spark.sql(\"create external table temp_tbl1(id string, name string) partitioned by(datekey string) stored as parquet location 'ext/temp_db/temp_tbl2'\")\n",
        "\n",
        " # 3. I ran this query 5+ times. :) It keeps adding new row because the drop table does not delete existing data due to the external table.\n",
        "spark.sql(\"insert into temp_tbl1 values('A1', 'Phill', '2020-01-30')\")\n",
        "\n",
        "spark.sql('select * from temp_tbl1').show()\n",
        "spark.sql('show partitions temp_tbl1').show()\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-----+----------+\n",
            "| id| name|   datekey|\n",
            "+---+-----+----------+\n",
            "| A1|Phill|2020-01-30|\n",
            "| A1|Phill|2020-01-30|\n",
            "+---+-----+----------+\n",
            "\n",
            "+------------------+\n",
            "|         partition|\n",
            "+------------------+\n",
            "|datekey=2020-01-30|\n",
            "+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vg6whMFKkKlw",
        "outputId": "a57cdba3-cc41-4882-a42d-0ed015f260d0"
      },
      "source": [
        "# 1. It creates a metastore_db and derby.log\n",
        "# Table or view 'temp_tbl2' already exists in database 'default'\n",
        "spark.sql('drop table if exists temp_tbl2') # It drops the table and delete the table folder too\n",
        "# 2. It creates a spark-warehouse/temp_tbl2/datekey=2021-06-20/_SUCCESS\n",
        "spark.sql(\"create table temp_tbl2(id string, name string) partitioned by(datekey string) stored as parquet\")\n",
        "# 3. It creates a partition automatically and adds parquet files. The last param value is considered as a partition value.\n",
        "spark.sql(\"insert into temp_tbl2 values('A1', 'Jugal', '2021-06-20')\") # It adds a part-00000-GUID.c000.snappy.parquet file.\n",
        "spark.sql(\"insert into temp_tbl2 values('A2', 'Garvik', '2021-06-20')\") # It adds an another part-00000-GUID.c000.snappy.parquet file.\n",
        "spark.sql(\"insert into temp_tbl2 values('A3', 'Jack', '2021-06-21')\") # It adds an another part-00000-GUID.c000.snappy.parquet file but in a new partition.\n",
        "\n",
        "spark.sql('select * from temp_tbl2').show() # DataFrame\n",
        "spark.sql('show partitions temp_tbl2').show()\n",
        "\n",
        "# 4 It adds new partition\n",
        "spark.sql(\"insert overwrite table temp_tbl2 partition(datekey) values('A4', 'Jill', '2021-06-22')\")\n",
        "spark.sql(\"insert into temp_tbl2 values('A5', 'Paul', '2021-06-22')\")\n",
        "spark.sql('select * from temp_tbl2').show()\n",
        "spark.sql('show partitions temp_tbl2').show()\n",
        "\n",
        "# 5 It adds new partition but overwrite the existing partition too.\n",
        "spark.sql(\"insert overwrite table temp_tbl2 partition(datekey) values('A6', 'Mark', '2021-06-22')\")\n",
        "spark.sql('select * from temp_tbl2').show()\n",
        "spark.sql('show partitions temp_tbl2').show()\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------+----------+\n",
            "| id|  name|   datekey|\n",
            "+---+------+----------+\n",
            "| A2|Garvik|2021-06-20|\n",
            "| A1| Jugal|2021-06-20|\n",
            "| A3|  Jack|2021-06-21|\n",
            "+---+------+----------+\n",
            "\n",
            "+------------------+\n",
            "|         partition|\n",
            "+------------------+\n",
            "|datekey=2021-06-20|\n",
            "|datekey=2021-06-21|\n",
            "+------------------+\n",
            "\n",
            "+---+------+----------+\n",
            "| id|  name|   datekey|\n",
            "+---+------+----------+\n",
            "| A2|Garvik|2021-06-20|\n",
            "| A1| Jugal|2021-06-20|\n",
            "| A3|  Jack|2021-06-21|\n",
            "| A5|  Paul|2021-06-22|\n",
            "| A4|  Jill|2021-06-22|\n",
            "+---+------+----------+\n",
            "\n",
            "+------------------+\n",
            "|         partition|\n",
            "+------------------+\n",
            "|datekey=2021-06-20|\n",
            "|datekey=2021-06-21|\n",
            "|datekey=2021-06-22|\n",
            "+------------------+\n",
            "\n",
            "+---+------+----------+\n",
            "| id|  name|   datekey|\n",
            "+---+------+----------+\n",
            "| A2|Garvik|2021-06-20|\n",
            "| A1| Jugal|2021-06-20|\n",
            "| A3|  Jack|2021-06-21|\n",
            "| A6|  Mark|2021-06-22|\n",
            "+---+------+----------+\n",
            "\n",
            "+------------------+\n",
            "|         partition|\n",
            "+------------------+\n",
            "|datekey=2021-06-20|\n",
            "|datekey=2021-06-21|\n",
            "|datekey=2021-06-22|\n",
            "+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOZl57yA4Rc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dac594d1-76da-4a16-cd91-7365514d1625"
      },
      "source": [
        "df_temp_tbl2 = spark.sql('select * from temp_tbl2')\n",
        "df_temp_tbl2.createOrReplaceTempView(\"temp_tbl3\")\n",
        "\n",
        "spark.sql('drop table if exists temp_tbl4')\n",
        "spark.sql(\"create table temp_tbl4(id string, name string) partitioned by(datekey string) stored as parquet\")\n",
        "\n",
        "spark.sql(\"insert overwrite table temp_tbl4 partition(datekey) select * from temp_tbl3\") # It's required to create a table first.\n",
        "\n",
        "spark.sql(\"select * from temp_tbl4\").show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------+----------+\n",
            "| id|  name|   datekey|\n",
            "+---+------+----------+\n",
            "| A3|  Jack|2021-06-21|\n",
            "| A6|  Mark|2021-06-22|\n",
            "| A2|Garvik|2021-06-20|\n",
            "| A1| Jugal|2021-06-20|\n",
            "+---+------+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obP01hFERf9m"
      },
      "source": [
        "###SQL-Hive Catalog"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3p0Li8_RD_1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5afecdf8-ecbd-4c6c-d06c-51ad380fa4a0"
      },
      "source": [
        "spark.catalog.listDatabases()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Database(name='default', description='Default Hive database', locationUri='file:/content/spark-warehouse')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3sVKxZ-gttZ",
        "outputId": "86968930-c19d-4ff0-df6a-9e4291b65f57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "spark.catalog.listTables()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Table(name='temp_tbl1', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
              " Table(name='temp_tbl2', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
              " Table(name='temp_tbl4', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
              " Table(name='people_json_tbl', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
              " Table(name='people_tbl', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
              " Table(name='temp_tbl3', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7YKirvEhJRA",
        "outputId": "1f438e71-e909-4a2b-d9e4-5362677d0f41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "spark.read.table(\"default.temp_tbl1\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: string, name: string, datekey: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    }
  ]
}