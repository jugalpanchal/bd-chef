{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spark_hive_ops.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPlTWKr3gbmdRQ650vcy2EQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jugalpanchal/bd-chef/blob/main/spark_hive_ops.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l71kit82f_ac"
      },
      "source": [
        "# Follow the steps to install the dependencies:\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # install java\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz # spark package download\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz # unzip spark package\n",
        "!pip install -q findspark # install spark\n",
        "\n",
        "# Set the location of Java and Spark:\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANgvtS-DgCtw"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# create or get spark session\n",
        "# If we don't specifiy the Hive support then it gives an error while creating a table:\n",
        "# AnalysisException: Hive support is required to CREATE Hive TABLE (AS SELECT);\n",
        "spark = SparkSession.builder \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .appName(\"Spark_App1\") \\\n",
        "        .enableHiveSupport() \\\n",
        "        .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Q9zUndRgGXd"
      },
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql import HiveContext\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "hc = HiveContext(sc) # create a Hive context\n",
        "sqc = SQLContext(sc) # create a SQL context\n",
        "\n",
        "# So does diff between these two? - \n",
        "# HiveContext is still the superset of SQLContext.\n",
        "# It contains certain extra properties such as it can read the configuration from hive-site.xml, in case you have hive use cases otherwise simply use SQLContext"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K13Th5iWtjMG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f41a7b5-9c3f-425f-abf9-e35b8a37f5e7"
      },
      "source": [
        "hc"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.context.HiveContext at 0x7f98df7364d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-SeMAcStlw0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "149de852-9954-4efb-f1de-6323b9678444"
      },
      "source": [
        "sqc"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.context.SQLContext at 0x7f98df736590>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaubDxXwmWnO"
      },
      "source": [
        "### Load Constant Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ev9O57Q7garT",
        "outputId": "14442a02-f3e8-4144-de22-88f97f64fdc9"
      },
      "source": [
        "people_list = [('A', 25), ('B', 20), ('C', 30), ('D', 15)]\n",
        "people_rdd_pair = sc.parallelize(people_list) # create a RDD but with Tuple/Pair objects\n",
        "#people_rdd_pair # ParallelCollectionRDD[2] at readRDDFromFile\n",
        "\n",
        "people_rdd_row = people_rdd_pair.map(lambda x: Row(name=x[0], age=int(x[1]))) # create a RDD with Row objects\n",
        "#people # PythonRDD[5] at RDD\n",
        "\n",
        "people_rdd_row.collect()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(name='A', age=25),\n",
              " Row(name='B', age=20),\n",
              " Row(name='C', age=30),\n",
              " Row(name='D', age=15)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MNk8Cg1jJKS",
        "outputId": "65496474-3bca-4b0e-c67b-f58bc25ce1b9"
      },
      "source": [
        "people_df = spark.createDataFrame(people_rdd_row)\n",
        "#people_df # DataFrame[name: string, age: bigint]\n",
        "\n",
        "people_df.collect()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(name='A', age=25),\n",
              " Row(name='B', age=20),\n",
              " Row(name='C', age=30),\n",
              " Row(name='D', age=15)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOJS3tpis6SB"
      },
      "source": [
        "### HiveContext"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8SdL9u1j2wr",
        "outputId": "e1f0727f-93a2-49ab-f763-8c56bb2214ab"
      },
      "source": [
        "hc.registerDataFrameAsTable(people_df, 'people_tbl') # create a table\n",
        "hc.sql('show tables').show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+----------+-----------+\n",
            "|database| tableName|isTemporary|\n",
            "+--------+----------+-----------+\n",
            "| default| temp_tbl1|      false|\n",
            "| default| temp_tbl2|      false|\n",
            "|        |people_tbl|       true|\n",
            "+--------+----------+-----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIFqPgBRljvy",
        "outputId": "d844124f-f663-412e-b867-684c24cb0aa6"
      },
      "source": [
        "hc.sql('select * from people_tbl').show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+---+\n",
            "|name|age|\n",
            "+----+---+\n",
            "|   A| 25|\n",
            "|   B| 20|\n",
            "|   C| 30|\n",
            "|   D| 15|\n",
            "+----+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vB6YVD5mO5d"
      },
      "source": [
        "###Load JSON Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1V39qcM4msDc",
        "outputId": "13863f3e-86c1-4616-e1cb-28627d52d295"
      },
      "source": [
        "people_json_df = spark.read.json('sample_data/anscombe.json') # Series is not Pandas class here :) It is a key in JSON file.\n",
        "people_json_df"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Series: string, X: double, Y: double, _corrupt_record: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkR900tZnnqj",
        "outputId": "be9299c8-0198-4d85-876e-8e7d98441802"
      },
      "source": [
        "people_json_df.registerTempTable('people_json_tbl')\n",
        "hc.sql('show tables').show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+---------------+-----------+\n",
            "|database|      tableName|isTemporary|\n",
            "+--------+---------------+-----------+\n",
            "| default|      temp_tbl1|      false|\n",
            "| default|      temp_tbl2|      false|\n",
            "|        |people_json_tbl|       true|\n",
            "|        |     people_tbl|       true|\n",
            "+--------+---------------+-----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dG67LtSLn0gu",
        "outputId": "5b2940c6-f2c7-4918-ecb1-3b99db5a0730"
      },
      "source": [
        "results = hc.sql('select * from people_json_tbl')\n",
        "results.show() # default shows top 20 rows"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+----+-----+---------------+\n",
            "|Series|   X|    Y|_corrupt_record|\n",
            "+------+----+-----+---------------+\n",
            "|  null|null| null|              [|\n",
            "|     I|10.0| 8.04|           null|\n",
            "|     I| 8.0| 6.95|           null|\n",
            "|     I|13.0| 7.58|           null|\n",
            "|     I| 9.0| 8.81|           null|\n",
            "|     I|11.0| 8.33|           null|\n",
            "|     I|14.0| 9.96|           null|\n",
            "|     I| 6.0| 7.24|           null|\n",
            "|     I| 4.0| 4.26|           null|\n",
            "|     I|12.0|10.84|           null|\n",
            "|     I| 7.0| 4.81|           null|\n",
            "|     I| 5.0| 5.68|           null|\n",
            "|    II|10.0| 9.14|           null|\n",
            "|    II| 8.0| 8.14|           null|\n",
            "|    II|13.0| 8.74|           null|\n",
            "|    II| 9.0| 8.77|           null|\n",
            "|    II|11.0| 9.26|           null|\n",
            "|    II|14.0|  8.1|           null|\n",
            "|    II| 6.0| 6.13|           null|\n",
            "|    II| 4.0|  3.1|           null|\n",
            "+------+----+-----+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7tnd8OrsnE2"
      },
      "source": [
        "### If we need to connect Hadoop Yarn Hive then make sure the script is submited in the Yarn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tolKXsfxo-PJ"
      },
      "source": [
        "# Set master as yarn in that case and enable hive support\n",
        "spark = SparkSession.builder \\\n",
        "        .master(\"yarn\") \\\n",
        "        .appName(\"Spark_App1\") \\\n",
        "        .enableHiveSupport() \\\n",
        "        .getOrCreate()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw1UU5aBsq3T"
      },
      "source": [
        "### SQLContext"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tI8dlwJctL8S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb1c6f03-a6f3-4c2b-deb1-171063b60341"
      },
      "source": [
        "# It's a alternative way to play with data. SQLContext helps to create a DataFrame too.\n",
        "\n",
        "df = sqc.range(5)\n",
        "#df # DataFrame[id: bigint]\n",
        "df.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  0|\n",
            "|  1|\n",
            "|  2|\n",
            "|  3|\n",
            "|  4|\n",
            "+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRtO_gffkKal"
      },
      "source": [
        "### Partition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vg6whMFKkKlw",
        "outputId": "d6a7e9f1-a827-4822-baaa-8728123a33e4"
      },
      "source": [
        "# 1. It creates a metastore_db and derby.log\n",
        "spark.sql('drop table if exists temp_tbl2') # It drops the table and delete the table folder too\n",
        "# 2. It creates a spark-warehouse/temp_tbl2/datekey=2021-06-20/_SUCCESS\n",
        "spark.sql(\"create table temp_tbl2(id string, name string) partitioned by(datekey string) stored as parquet\")\n",
        "# 3. It creates a partition automatically and adds parquet files.\n",
        "spark.sql(\"insert into temp_tbl2 values('A1', 'Jugal', '2021-06-20')\") # It adds a part-00000-GUID.c000.snappy.parquet file.\n",
        "spark.sql(\"insert into temp_tbl2 values('A2', 'Garvik', '2021-06-20')\") # It adds an another part-00000-GUID.c000.snappy.parquet file.\n",
        "spark.sql(\"insert into temp_tbl2 values('A3', 'Jack', '2021-06-21')\") # It adds an another part-00000-GUID.c000.snappy.parquet file but in a new partition.\n",
        "\n",
        "spark.sql('select * from temp_tbl2').show() # DataFrame\n",
        "spark.sql('show partitions temp_tbl2').show()\n",
        "\n",
        "# 4 It adds new partition\n",
        "spark.sql(\"insert overwrite table temp_tbl2 partition(datekey) values('A4', 'Jill', '2021-06-22')\")\n",
        "spark.sql(\"insert into temp_tbl2 values('A5', 'Paul', '2021-06-22')\")\n",
        "spark.sql('select * from temp_tbl2').show()\n",
        "spark.sql('show partitions temp_tbl2').show()\n",
        "\n",
        "# 5 It adds new partition but overwrite the existing partition too.\n",
        "spark.sql(\"insert overwrite table temp_tbl2 partition(datekey) values('A6', 'Mark', '2021-06-22')\")\n",
        "spark.sql('select * from temp_tbl2').show()\n",
        "spark.sql('show partitions temp_tbl2').show()\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------+----------+\n",
            "| id|  name|   datekey|\n",
            "+---+------+----------+\n",
            "| A2|Garvik|2021-06-20|\n",
            "| A1| Jugal|2021-06-20|\n",
            "| A3|  Jack|2021-06-21|\n",
            "+---+------+----------+\n",
            "\n",
            "+------------------+\n",
            "|         partition|\n",
            "+------------------+\n",
            "|datekey=2021-06-20|\n",
            "|datekey=2021-06-21|\n",
            "+------------------+\n",
            "\n",
            "+---+------+----------+\n",
            "| id|  name|   datekey|\n",
            "+---+------+----------+\n",
            "| A2|Garvik|2021-06-20|\n",
            "| A1| Jugal|2021-06-20|\n",
            "| A3|  Jack|2021-06-21|\n",
            "| A4|  Jill|2021-06-22|\n",
            "| A5|  Paul|2021-06-22|\n",
            "+---+------+----------+\n",
            "\n",
            "+------------------+\n",
            "|         partition|\n",
            "+------------------+\n",
            "|datekey=2021-06-20|\n",
            "|datekey=2021-06-21|\n",
            "|datekey=2021-06-22|\n",
            "+------------------+\n",
            "\n",
            "+---+------+----------+\n",
            "| id|  name|   datekey|\n",
            "+---+------+----------+\n",
            "| A2|Garvik|2021-06-20|\n",
            "| A1| Jugal|2021-06-20|\n",
            "| A3|  Jack|2021-06-21|\n",
            "| A6|  Mark|2021-06-22|\n",
            "+---+------+----------+\n",
            "\n",
            "+------------------+\n",
            "|         partition|\n",
            "+------------------+\n",
            "|datekey=2021-06-20|\n",
            "|datekey=2021-06-21|\n",
            "|datekey=2021-06-22|\n",
            "+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1N1u3YqCqFKo",
        "outputId": "6077e139-9237-45f4-da4f-20d457d5dc38"
      },
      "source": [
        "# 1. It won't delete the files and folders. because it is an external table.\n",
        "spark.sql('drop table if exists temp_tbl1')\n",
        "# 2. It creates a spark-warehouse/ext/temp_db/temp_tbl2/datekey=2021-06-20/_SUCCESS\n",
        "# CREATE EXTERNAL TABLE must be accompanied by LOCATION\n",
        "spark.sql(\"create external table temp_tbl1(id string, name string) partitioned by(datekey string) stored as parquet location 'ext/temp_db/temp_tbl2'\")\n",
        "\n",
        " # 3. I ran this query 5+ times. :) It keeps adding new row because the drop table does not delete existing data due to the external table.\n",
        "spark.sql(\"insert into temp_tbl1 values('A1', 'Phill', '2020-01-30')\")\n",
        "\n",
        "spark.sql('select * from temp_tbl1').show()\n",
        "spark.sql('show partitions temp_tbl1').show()\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-----+----------+\n",
            "| id| name|   datekey|\n",
            "+---+-----+----------+\n",
            "| A1|Phill|2020-01-30|\n",
            "| A1|Phill|2020-01-30|\n",
            "| A1|Phill|2020-01-30|\n",
            "| A1|Phill|2020-01-30|\n",
            "| A1|Phill|2020-01-30|\n",
            "| A1|Phill|2020-01-30|\n",
            "+---+-----+----------+\n",
            "\n",
            "+------------------+\n",
            "|         partition|\n",
            "+------------------+\n",
            "|datekey=2020-01-30|\n",
            "+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOZl57yA4Rc2"
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    }
  ]
}