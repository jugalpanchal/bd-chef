{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spark_hive_ops.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMOpsfR0uFuyHi89V/e6U2c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jugalpanchal/bd-chef/blob/main/spark_hive_ops.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l71kit82f_ac"
      },
      "source": [
        "# Follow the steps to install the dependencies:\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # install java\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz # spark package download\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz # unzip spark package\n",
        "!pip install -q findspark # install spark\n",
        "\n",
        "# Set the location of Java and Spark:\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANgvtS-DgCtw"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# create or get spark session\n",
        "# If we don't specifiy the Hive support then it gives an error while creating a table:\n",
        "# AnalysisException: Hive support is required to CREATE Hive TABLE (AS SELECT);\n",
        "spark = SparkSession.builder \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .appName(\"Spark_App1\") \\\n",
        "        .enableHiveSupport() \\\n",
        "        .getOrCreate()\n",
        "        \n",
        "        #.config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
        "        #.config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
        "\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Q9zUndRgGXd"
      },
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql import HiveContext\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "hc = HiveContext(sc) # create a Hive context\n",
        "sqc = SQLContext(sc) # create a SQL context\n",
        "\n",
        "# So does diff between these two? - \n",
        "# HiveContext is still the superset of SQLContext.\n",
        "# It contains certain extra properties such as it can read the configuration from hive-site.xml, in case you have hive use cases otherwise simply use SQLContext\n",
        "\n",
        "# The sql is an alternative of the DataFrame DSL(Domain Specfic Language ex Map, GroupBy, FlatMap etc.)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K13Th5iWtjMG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27e6bfe0-69f4-4adc-c171-2553be8029d9"
      },
      "source": [
        "hc"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.context.HiveContext at 0x7fd9d1c62310>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-SeMAcStlw0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04038aa9-b3c9-46e2-d33c-ac65de795794"
      },
      "source": [
        "sqc"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.context.SQLContext at 0x7fd9d1ef5950>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaubDxXwmWnO"
      },
      "source": [
        "### Load Constant Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ev9O57Q7garT",
        "outputId": "4b0024e7-7904-4cae-9b23-f9e65c95ec15"
      },
      "source": [
        "people_list = [('A', 25), ('B', 20), ('C', 30), ('D', 15)]\n",
        "people_rdd_pair = sc.parallelize(people_list) # create a RDD but with Tuple/Pair objects\n",
        "#people_rdd_pair # ParallelCollectionRDD[2] at readRDDFromFile\n",
        "\n",
        "people_rdd_row = people_rdd_pair.map(lambda x: Row(name=x[0], age=int(x[1]))) # create a RDD with Row objects\n",
        "#people # PythonRDD[5] at RDD\n",
        "\n",
        "people_rdd_row.collect()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(name='A', age=25),\n",
              " Row(name='B', age=20),\n",
              " Row(name='C', age=30),\n",
              " Row(name='D', age=15)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MNk8Cg1jJKS",
        "outputId": "04a387cf-3838-4f4a-b869-eff48a89456d"
      },
      "source": [
        "people_df = spark.createDataFrame(people_rdd_row)\n",
        "#people_df # DataFrame[name: string, age: bigint]\n",
        "\n",
        "people_df.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+---+\n",
            "|name|age|\n",
            "+----+---+\n",
            "|   A| 25|\n",
            "|   B| 20|\n",
            "|   C| 30|\n",
            "|   D| 15|\n",
            "+----+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOJS3tpis6SB"
      },
      "source": [
        "### HiveContext"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8SdL9u1j2wr"
      },
      "source": [
        "hc.registerDataFrameAsTable(people_df, 'people_tbl') # create a table"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7q6o9gAlrzX_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88864547-ff82-496c-f485-aeaad2a0c6f7"
      },
      "source": [
        "hc.sql('show databases').show()\n",
        "spark.catalog.listDatabases()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|namespace|\n",
            "+---------+\n",
            "|  default|\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Database(name='default', description='Default Hive database', locationUri='file:/content/spark-warehouse')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3rvdY4JsS0E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8797345-26bf-47f9-85a5-7b1598c3fb30"
      },
      "source": [
        "hc.sql('show tables').show()\n",
        "spark.catalog.listTables()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+----------+-----------+\n",
            "|database| tableName|isTemporary|\n",
            "+--------+----------+-----------+\n",
            "| default| temp_tbl1|      false|\n",
            "| default| temp_tbl2|      false|\n",
            "| default| temp_tbl4|      false|\n",
            "| default|temp_tbl41|      false|\n",
            "|        |people_tbl|       true|\n",
            "+--------+----------+-----------+\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Table(name='temp_tbl1', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
              " Table(name='temp_tbl2', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
              " Table(name='temp_tbl4', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
              " Table(name='temp_tbl41', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
              " Table(name='people_tbl', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIFqPgBRljvy",
        "outputId": "ce9fa377-8e65-4740-ef05-83bc508e8931"
      },
      "source": [
        "hc.sql('select * from people_tbl').show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+---+\n",
            "|name|age|\n",
            "+----+---+\n",
            "|   A| 25|\n",
            "|   B| 20|\n",
            "|   C| 30|\n",
            "|   D| 15|\n",
            "+----+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vB6YVD5mO5d"
      },
      "source": [
        "###Load JSON Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1V39qcM4msDc",
        "outputId": "5d426d80-9718-44a8-a473-b88f7be163ed"
      },
      "source": [
        "people_json_df = spark.read.json('sample_data/anscombe.json') # Series is not Pandas class here :) It is a key in JSON file.\n",
        "people_json_df"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Series: string, X: double, Y: double, _corrupt_record: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkR900tZnnqj",
        "outputId": "4d7f1a31-e30a-4851-a8e4-916c450b7bc2"
      },
      "source": [
        "people_json_df.registerTempTable('people_json_tbl')\n",
        "hc.sql('show tables').show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+---------------+-----------+\n",
            "|database|      tableName|isTemporary|\n",
            "+--------+---------------+-----------+\n",
            "| default|      temp_tbl1|      false|\n",
            "| default|      temp_tbl2|      false|\n",
            "| default|      temp_tbl4|      false|\n",
            "| default|     temp_tbl41|      false|\n",
            "|        |people_json_tbl|       true|\n",
            "|        |     people_tbl|       true|\n",
            "+--------+---------------+-----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dG67LtSLn0gu",
        "outputId": "e1f4cd34-4e1a-4d88-da73-ccccfd6b6b65"
      },
      "source": [
        "results = hc.sql('select * from people_json_tbl')\n",
        "results.show() # default shows top 20 rows"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+----+-----+---------------+\n",
            "|Series|   X|    Y|_corrupt_record|\n",
            "+------+----+-----+---------------+\n",
            "|  null|null| null|              [|\n",
            "|     I|10.0| 8.04|           null|\n",
            "|     I| 8.0| 6.95|           null|\n",
            "|     I|13.0| 7.58|           null|\n",
            "|     I| 9.0| 8.81|           null|\n",
            "|     I|11.0| 8.33|           null|\n",
            "|     I|14.0| 9.96|           null|\n",
            "|     I| 6.0| 7.24|           null|\n",
            "|     I| 4.0| 4.26|           null|\n",
            "|     I|12.0|10.84|           null|\n",
            "|     I| 7.0| 4.81|           null|\n",
            "|     I| 5.0| 5.68|           null|\n",
            "|    II|10.0| 9.14|           null|\n",
            "|    II| 8.0| 8.14|           null|\n",
            "|    II|13.0| 8.74|           null|\n",
            "|    II| 9.0| 8.77|           null|\n",
            "|    II|11.0| 9.26|           null|\n",
            "|    II|14.0|  8.1|           null|\n",
            "|    II| 6.0| 6.13|           null|\n",
            "|    II| 4.0|  3.1|           null|\n",
            "+------+----+-----+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw1UU5aBsq3T"
      },
      "source": [
        "### SQLContext"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tI8dlwJctL8S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "add63cd9-db0e-482c-f0cc-7e21a187fcca"
      },
      "source": [
        "# It's a alternative way to play with data. SQLContext helps to create a DataFrame too.\n",
        "\n",
        "df = sqc.range(5)\n",
        "#df # DataFrame[id: bigint]\n",
        "df.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  0|\n",
            "|  1|\n",
            "|  2|\n",
            "|  3|\n",
            "|  4|\n",
            "+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRtO_gffkKal"
      },
      "source": [
        "### Partition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1N1u3YqCqFKo",
        "outputId": "af4fda4c-4328-4d4c-9e95-78f144fee095"
      },
      "source": [
        "# 1. It won't delete the files and folders. because it is an external table.\n",
        "# Table or view 'temp_tbl1' already exists in database 'default'\n",
        "spark.sql('drop table if exists temp_tbl1')\n",
        "# 2. It creates a spark-warehouse/ext/temp_db/temp_tbl2/datekey=2021-06-20/_SUCCESS\n",
        "# CREATE EXTERNAL TABLE must be accompanied by LOCATION\n",
        "spark.sql(\"create external table temp_tbl1(id string, name string) partitioned by(datekey string) stored as parquet location 'ext/temp_db/temp_tbl2'\")\n",
        "\n",
        " # 3. I ran this query 5+ times. :) It keeps adding new row because the drop table does not delete existing data due to the external table.\n",
        "spark.sql(\"insert into temp_tbl1 values('A1', 'Phill', '2020-01-30')\")\n",
        "\n",
        "spark.sql('select * from temp_tbl1').show()\n",
        "spark.sql('show partitions temp_tbl1').show()\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-----+----------+\n",
            "| id| name|   datekey|\n",
            "+---+-----+----------+\n",
            "| A1|Phill|2020-01-30|\n",
            "| A1|Phill|2020-01-30|\n",
            "| A1|Phill|2020-01-30|\n",
            "| A1|Phill|2020-01-30|\n",
            "+---+-----+----------+\n",
            "\n",
            "+------------------+\n",
            "|         partition|\n",
            "+------------------+\n",
            "|datekey=2020-01-30|\n",
            "+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vg6whMFKkKlw",
        "outputId": "e47137d4-dcf1-406f-d61b-dce952d65112"
      },
      "source": [
        "# 1. It creates a metastore_db and derby.log\n",
        "# Table or view 'temp_tbl2' already exists in database 'default'\n",
        "spark.sql('drop table if exists temp_tbl2') # It drops the table and delete the table folder too\n",
        "# 2. It creates a spark-warehouse/temp_tbl2/datekey=2021-06-20/_SUCCESS\n",
        "spark.sql(\"create table temp_tbl2(id string, name string) partitioned by(datekey string) stored as parquet\")\n",
        "# 3. It creates a partition automatically and adds parquet files. The last param value is considered as a partition value.\n",
        "spark.sql(\"insert into temp_tbl2 values('A1', 'Jugal', '2021-06-20')\") # It adds a part-00000-GUID.c000.snappy.parquet file.\n",
        "spark.sql(\"insert into temp_tbl2 values('A2', 'Garvik', '2021-06-20')\") # It adds an another part-00000-GUID.c000.snappy.parquet file.\n",
        "spark.sql(\"insert into temp_tbl2 values('A3', 'Jack', '2021-06-21')\") # It adds an another part-00000-GUID.c000.snappy.parquet file but in a new partition.\n",
        "\n",
        "spark.sql('select * from temp_tbl2').show() # DataFrame\n",
        "spark.sql('show partitions temp_tbl2').show()\n",
        "\n",
        "# 4 It adds new partition\n",
        "spark.sql(\"insert overwrite table temp_tbl2 partition(datekey) values('A4', 'Jill', '2021-06-22')\")\n",
        "spark.sql(\"insert into temp_tbl2 values('A5', 'Paul', '2021-06-22')\")\n",
        "spark.sql('select * from temp_tbl2').show()\n",
        "spark.sql('show partitions temp_tbl2').show()\n",
        "\n",
        "# 5 It adds new partition but overwrite the existing partition too.\n",
        "spark.sql(\"insert overwrite table temp_tbl2 partition(datekey) values('A6', 'Mark', '2021-06-22')\")\n",
        "spark.sql('select * from temp_tbl2').show()\n",
        "spark.sql('show partitions temp_tbl2').show()\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------+----------+\n",
            "| id|  name|   datekey|\n",
            "+---+------+----------+\n",
            "| A2|Garvik|2021-06-20|\n",
            "| A1| Jugal|2021-06-20|\n",
            "| A3|  Jack|2021-06-21|\n",
            "+---+------+----------+\n",
            "\n",
            "+------------------+\n",
            "|         partition|\n",
            "+------------------+\n",
            "|datekey=2021-06-20|\n",
            "|datekey=2021-06-21|\n",
            "+------------------+\n",
            "\n",
            "+---+------+----------+\n",
            "| id|  name|   datekey|\n",
            "+---+------+----------+\n",
            "| A2|Garvik|2021-06-20|\n",
            "| A1| Jugal|2021-06-20|\n",
            "| A3|  Jack|2021-06-21|\n",
            "| A5|  Paul|2021-06-22|\n",
            "| A4|  Jill|2021-06-22|\n",
            "+---+------+----------+\n",
            "\n",
            "+------------------+\n",
            "|         partition|\n",
            "+------------------+\n",
            "|datekey=2021-06-20|\n",
            "|datekey=2021-06-21|\n",
            "|datekey=2021-06-22|\n",
            "+------------------+\n",
            "\n",
            "+---+------+----------+\n",
            "| id|  name|   datekey|\n",
            "+---+------+----------+\n",
            "| A2|Garvik|2021-06-20|\n",
            "| A1| Jugal|2021-06-20|\n",
            "| A3|  Jack|2021-06-21|\n",
            "| A6|  Mark|2021-06-22|\n",
            "+---+------+----------+\n",
            "\n",
            "+------------------+\n",
            "|         partition|\n",
            "+------------------+\n",
            "|datekey=2021-06-20|\n",
            "|datekey=2021-06-21|\n",
            "|datekey=2021-06-22|\n",
            "+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOZl57yA4Rc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c6b4e35-e4a5-4ff9-f5d2-515e3b2a1141"
      },
      "source": [
        " # Case1: Create a table from a dataframe - A datekey column have to be in the dataframe.\n",
        " # Case2: Create a table from files - A datekey is not required to be in the file.\n",
        "df_temp_tbl2 = spark.sql('select * from temp_tbl2')\n",
        "df_temp_tbl2.show()\n",
        "\n",
        "df_temp_tbl2.createOrReplaceTempView(\"temp_tbl3\")\n",
        "\n",
        "spark.sql('drop table if exists temp_tbl4')\n",
        "spark.sql(\"create table temp_tbl4(id string, name string) partitioned by(datekey string) stored as parquet\")\n",
        "# Note: In case of the textfile(stored as textfile) it would give an exception. But the parquet won't give the exception.\n",
        "# org.apache.spark.SparkException: Dynamic partition strict mode requires at least one static partition column. To turn this off set hive.exec.dynamic.partition.mode=nonstrict\n",
        "\n",
        "# It's required to create a table first.\n",
        "# It won't write a datekey in the physical files along with data.\n",
        "spark.sql(\"insert overwrite table temp_tbl4 partition(datekey) select * from temp_tbl3\")\n",
        "\n",
        "spark.sql(\"select * from temp_tbl4\").show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------+----------+\n",
            "| id|  name|   datekey|\n",
            "+---+------+----------+\n",
            "| A2|Garvik|2021-06-20|\n",
            "| A1| Jugal|2021-06-20|\n",
            "| A3|  Jack|2021-06-21|\n",
            "| A6|  Mark|2021-06-22|\n",
            "+---+------+----------+\n",
            "\n",
            "+---+------+----------+\n",
            "| id|  name|   datekey|\n",
            "+---+------+----------+\n",
            "| A3|  Jack|2021-06-21|\n",
            "| A6|  Mark|2021-06-22|\n",
            "| A2|Garvik|2021-06-20|\n",
            "| A1| Jugal|2021-06-20|\n",
            "+---+------+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obP01hFERf9m"
      },
      "source": [
        "###SQL-Hive Catalog"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3p0Li8_RD_1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5d2aa87-a8d8-4db1-dfef-3675765bfd46"
      },
      "source": [
        "spark.catalog.listDatabases()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Database(name='default', description='Default Hive database', locationUri='file:/content/spark-warehouse')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3sVKxZ-gttZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f328fc50-90b0-4948-9416-ea73e4dfd5da"
      },
      "source": [
        "spark.catalog.listTables()\n",
        "# temp_tbl1, temp_tbl2, temp_tbl4 -> spark.sql(create external table or create table)\n",
        "\n",
        "# people_tbl -> HiveContext.registerDataFrameAsTable, \n",
        "# people_json_tbl -> dataframe.registerTempTable\n",
        "# temp_tbl3 -> dataframe.createOrReplaceTempView"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Table(name='temp_tbl1', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
              " Table(name='temp_tbl2', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
              " Table(name='temp_tbl4', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
              " Table(name='temp_tbl41', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
              " Table(name='people_json_tbl', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
              " Table(name='people_tbl', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
              " Table(name='temp_tbl3', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7YKirvEhJRA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94107196-0f34-44e2-a6ef-0f8edaaa00b2"
      },
      "source": [
        "spark.read.table(\"default.temp_tbl1\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: string, name: string, datekey: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vqkY8NAG1hS"
      },
      "source": [
        "Practices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NMPkTNdG30A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d4d365e-2a4f-46c5-d685-00d6e84654d0"
      },
      "source": [
        "# Get Max partition from the table\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "partition_df = spark.sql(\"select * from temp_tbl4\")\n",
        "partition_df.show()\n",
        "\n",
        "partition_df = spark.sql(\"show partitions temp_tbl4\")\n",
        "partition_df.show()\n",
        "\n",
        "partition_df.agg(F.max(partition_df.partition)).show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------+----------+\n",
            "| id|  name|   datekey|\n",
            "+---+------+----------+\n",
            "| A3|  Jack|2021-06-21|\n",
            "| A6|  Mark|2021-06-22|\n",
            "| A2|Garvik|2021-06-20|\n",
            "| A1| Jugal|2021-06-20|\n",
            "+---+------+----------+\n",
            "\n",
            "+------------------+\n",
            "|         partition|\n",
            "+------------------+\n",
            "|datekey=2021-06-20|\n",
            "|datekey=2021-06-21|\n",
            "|datekey=2021-06-22|\n",
            "+------------------+\n",
            "\n",
            "+------------------+\n",
            "|    max(partition)|\n",
            "+------------------+\n",
            "|datekey=2021-06-22|\n",
            "+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7tnd8OrsnE2"
      },
      "source": [
        "### If we need to connect Hadoop Yarn Hive then make sure the script is submited in the Yarn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tolKXsfxo-PJ"
      },
      "source": [
        "# Set master as yarn in that case and enable hive support\n",
        "spark = SparkSession.builder \\\n",
        "        .master(\"yarn\") \\\n",
        "        .appName(\"Spark_App1\") \\\n",
        "        .enableHiveSupport() \\\n",
        "        .getOrCreate()"
      ],
      "execution_count": 28,
      "outputs": []
    }
  ]
}