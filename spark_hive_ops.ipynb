{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spark_hive_ops.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOpzR2E287xQDbKHabcEwzx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jugalpanchal/bd-chef/blob/main/spark_hive_ops.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l71kit82f_ac"
      },
      "source": [
        "# Follow the steps to install the dependencies:\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # install java\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz # spark package download\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz # unzip spark package\n",
        "!pip install -q findspark # install spark\n",
        "\n",
        "# Set the location of Java and Spark:\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANgvtS-DgCtw"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# create or get spark session\n",
        "# If we don't specifiy the Hive support then it gives an error while creating a table:\n",
        "# AnalysisException: Hive support is required to CREATE Hive TABLE (AS SELECT);\n",
        "spark = SparkSession.builder \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .appName(\"Spark_App1\") \\\n",
        "        .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
        "        .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
        "        .enableHiveSupport() \\\n",
        "        .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Q9zUndRgGXd"
      },
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql import HiveContext\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "hc = HiveContext(sc) # create a Hive context\n",
        "sqc = SQLContext(sc) # create a SQL context\n",
        "\n",
        "# So does diff between these two? - \n",
        "# HiveContext is still the superset of SQLContext.\n",
        "# It contains certain extra properties such as it can read the configuration from hive-site.xml, in case you have hive use cases otherwise simply use SQLContext\n",
        "\n",
        "# The sql is an alternative of the DataFrame DSL(Domain Specfic Language ex Map, GroupBy, FlatMap etc.)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K13Th5iWtjMG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "296c0875-ab9b-40a2-de8d-af915c8eb174"
      },
      "source": [
        "hc"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.context.HiveContext at 0x7f1ef653fbd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-SeMAcStlw0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1481650c-836e-4708-bdc1-1146d489cdca"
      },
      "source": [
        "sqc"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.context.SQLContext at 0x7f1ef653ff50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaubDxXwmWnO"
      },
      "source": [
        "### Load Constant Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ev9O57Q7garT",
        "outputId": "98d9be73-56e8-4f85-bb3a-397f5e2109c8"
      },
      "source": [
        "people_list = [('A', 25), ('B', 20), ('C', 30), ('D', 15)]\n",
        "people_rdd_pair = sc.parallelize(people_list) # create a RDD but with Tuple/Pair objects\n",
        "#people_rdd_pair # ParallelCollectionRDD[2] at readRDDFromFile\n",
        "\n",
        "people_rdd_row = people_rdd_pair.map(lambda x: Row(name=x[0], age=int(x[1]))) # create a RDD with Row objects\n",
        "#people # PythonRDD[5] at RDD\n",
        "\n",
        "people_rdd_row.collect()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(name='A', age=25),\n",
              " Row(name='B', age=20),\n",
              " Row(name='C', age=30),\n",
              " Row(name='D', age=15)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MNk8Cg1jJKS",
        "outputId": "c1396c12-7a11-4d9f-9a7e-58adb15716f8"
      },
      "source": [
        "people_df = spark.createDataFrame(people_rdd_row)\n",
        "#people_df # DataFrame[name: string, age: bigint]\n",
        "\n",
        "people_df.show()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+---+\n",
            "|name|age|\n",
            "+----+---+\n",
            "|   A| 25|\n",
            "|   B| 20|\n",
            "|   C| 30|\n",
            "|   D| 15|\n",
            "+----+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOJS3tpis6SB"
      },
      "source": [
        "### HiveContext"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8SdL9u1j2wr"
      },
      "source": [
        "hc.registerDataFrameAsTable(people_df, 'people_tbl') # create a table"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7q6o9gAlrzX_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "082a3645-341d-4a2a-fe8f-f829587b4738"
      },
      "source": [
        "hc.sql('show databases').show()\n",
        "spark.catalog.listDatabases()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|namespace|\n",
            "+---------+\n",
            "|  default|\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Database(name='default', description='Default Hive database', locationUri='file:/content/spark-warehouse')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3rvdY4JsS0E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dde051ca-0f90-4be9-88f9-3e826e1975ef"
      },
      "source": [
        "hc.sql('show tables').show()\n",
        "spark.catalog.listTables()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+---------------+-----------+\n",
            "|database|      tableName|isTemporary|\n",
            "+--------+---------------+-----------+\n",
            "| default|      temp_tbl1|      false|\n",
            "| default|      temp_tbl2|      false|\n",
            "| default|      temp_tbl4|      false|\n",
            "| default|     temp_tbl41|      false|\n",
            "|        |people_json_tbl|       true|\n",
            "|        |     people_tbl|       true|\n",
            "|        |      temp_tbl3|       true|\n",
            "+--------+---------------+-----------+\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Table(name='temp_tbl1', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
              " Table(name='temp_tbl2', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
              " Table(name='temp_tbl4', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
              " Table(name='temp_tbl41', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
              " Table(name='people_json_tbl', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
              " Table(name='people_tbl', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
              " Table(name='temp_tbl3', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIFqPgBRljvy",
        "outputId": "589669e0-5e45-4b09-c62e-fc69951d0456"
      },
      "source": [
        "hc.sql('select * from people_tbl').show()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+---+\n",
            "|name|age|\n",
            "+----+---+\n",
            "|   A| 25|\n",
            "|   B| 20|\n",
            "|   C| 30|\n",
            "|   D| 15|\n",
            "+----+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vB6YVD5mO5d"
      },
      "source": [
        "###Load JSON Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1V39qcM4msDc",
        "outputId": "df32af2a-119f-4c1e-84b5-5fd3e9c17c1a"
      },
      "source": [
        "people_json_df = spark.read.json('sample_data/anscombe.json') # Series is not Pandas class here :) It is a key in JSON file.\n",
        "people_json_df"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Series: string, X: double, Y: double, _corrupt_record: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkR900tZnnqj",
        "outputId": "d6a9a3df-e727-48e0-c180-17f650b7aa49"
      },
      "source": [
        "people_json_df.registerTempTable('people_json_tbl')\n",
        "hc.sql('show tables').show()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+---------------+-----------+\n",
            "|database|      tableName|isTemporary|\n",
            "+--------+---------------+-----------+\n",
            "| default|      temp_tbl1|      false|\n",
            "| default|      temp_tbl2|      false|\n",
            "| default|      temp_tbl4|      false|\n",
            "| default|     temp_tbl41|      false|\n",
            "|        |people_json_tbl|       true|\n",
            "|        |     people_tbl|       true|\n",
            "|        |      temp_tbl3|       true|\n",
            "+--------+---------------+-----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dG67LtSLn0gu",
        "outputId": "255354a4-4594-47bb-8c21-f18f6b697a1c"
      },
      "source": [
        "results = hc.sql('select * from people_json_tbl')\n",
        "results.show() # default shows top 20 rows"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+----+-----+---------------+\n",
            "|Series|   X|    Y|_corrupt_record|\n",
            "+------+----+-----+---------------+\n",
            "|  null|null| null|              [|\n",
            "|     I|10.0| 8.04|           null|\n",
            "|     I| 8.0| 6.95|           null|\n",
            "|     I|13.0| 7.58|           null|\n",
            "|     I| 9.0| 8.81|           null|\n",
            "|     I|11.0| 8.33|           null|\n",
            "|     I|14.0| 9.96|           null|\n",
            "|     I| 6.0| 7.24|           null|\n",
            "|     I| 4.0| 4.26|           null|\n",
            "|     I|12.0|10.84|           null|\n",
            "|     I| 7.0| 4.81|           null|\n",
            "|     I| 5.0| 5.68|           null|\n",
            "|    II|10.0| 9.14|           null|\n",
            "|    II| 8.0| 8.14|           null|\n",
            "|    II|13.0| 8.74|           null|\n",
            "|    II| 9.0| 8.77|           null|\n",
            "|    II|11.0| 9.26|           null|\n",
            "|    II|14.0|  8.1|           null|\n",
            "|    II| 6.0| 6.13|           null|\n",
            "|    II| 4.0|  3.1|           null|\n",
            "+------+----+-----+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw1UU5aBsq3T"
      },
      "source": [
        "### SQLContext"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tI8dlwJctL8S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0b49db9-c022-4146-a9b6-823c760e30a9"
      },
      "source": [
        "# It's a alternative way to play with data. SQLContext helps to create a DataFrame too.\n",
        "\n",
        "df = sqc.range(5)\n",
        "#df # DataFrame[id: bigint]\n",
        "df.show()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  0|\n",
            "|  1|\n",
            "|  2|\n",
            "|  3|\n",
            "|  4|\n",
            "+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRtO_gffkKal"
      },
      "source": [
        "### Partition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1N1u3YqCqFKo",
        "outputId": "26684b7d-4260-40a1-918a-958373ea78c4"
      },
      "source": [
        "# 1. It won't delete the files and folders. because it is an external table.\n",
        "# Table or view 'temp_tbl1' already exists in database 'default'\n",
        "spark.sql('drop table if exists temp_tbl1')\n",
        "# 2. It creates a spark-warehouse/ext/temp_db/temp_tbl2/datekey=2021-06-20/_SUCCESS\n",
        "# CREATE EXTERNAL TABLE must be accompanied by LOCATION\n",
        "spark.sql(\"create external table temp_tbl1(id string, name string) partitioned by(datekey string) stored as parquet location 'ext/temp_db/temp_tbl2'\")\n",
        "\n",
        " # 3. I ran this query 5+ times. :) It keeps adding new row because the drop table does not delete existing data due to the external table.\n",
        "spark.sql(\"insert into temp_tbl1 values('A1', 'Phill', '2020-01-30')\")\n",
        "\n",
        "spark.sql('select * from temp_tbl1').show()\n",
        "spark.sql('show partitions temp_tbl1').show()\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-----+----------+\n",
            "| id| name|   datekey|\n",
            "+---+-----+----------+\n",
            "| A1|Phill|2020-01-30|\n",
            "| A1|Phill|2020-01-30|\n",
            "| A1|Phill|2020-01-30|\n",
            "+---+-----+----------+\n",
            "\n",
            "+------------------+\n",
            "|         partition|\n",
            "+------------------+\n",
            "|datekey=2020-01-30|\n",
            "+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vg6whMFKkKlw",
        "outputId": "770327d3-6743-45e9-ead5-94b61be8afa1"
      },
      "source": [
        "# 1. It creates a metastore_db and derby.log\n",
        "# Table or view 'temp_tbl2' already exists in database 'default'\n",
        "spark.sql('drop table if exists temp_tbl2') # It drops the table and delete the table folder too\n",
        "# 2. It creates a spark-warehouse/temp_tbl2/datekey=2021-06-20/_SUCCESS\n",
        "spark.sql(\"create table temp_tbl2(id string, name string) partitioned by(datekey string) stored as parquet\")\n",
        "# 3. It creates a partition automatically and adds parquet files. The last param value is considered as a partition value.\n",
        "spark.sql(\"insert into temp_tbl2 values('A1', 'Jugal', '2021-06-20')\") # It adds a part-00000-GUID.c000.snappy.parquet file.\n",
        "spark.sql(\"insert into temp_tbl2 values('A2', 'Garvik', '2021-06-20')\") # It adds an another part-00000-GUID.c000.snappy.parquet file.\n",
        "spark.sql(\"insert into temp_tbl2 values('A3', 'Jack', '2021-06-21')\") # It adds an another part-00000-GUID.c000.snappy.parquet file but in a new partition.\n",
        "\n",
        "spark.sql('select * from temp_tbl2').show() # DataFrame\n",
        "spark.sql('show partitions temp_tbl2').show()\n",
        "\n",
        "# 4 It adds new partition\n",
        "spark.sql(\"insert overwrite table temp_tbl2 partition(datekey) values('A4', 'Jill', '2021-06-22')\")\n",
        "spark.sql(\"insert into temp_tbl2 values('A5', 'Paul', '2021-06-22')\")\n",
        "spark.sql('select * from temp_tbl2').show()\n",
        "spark.sql('show partitions temp_tbl2').show()\n",
        "\n",
        "# 5 It adds new partition but overwrite the existing partition too.\n",
        "spark.sql(\"insert overwrite table temp_tbl2 partition(datekey) values('A6', 'Mark', '2021-06-22')\")\n",
        "spark.sql('select * from temp_tbl2').show()\n",
        "spark.sql('show partitions temp_tbl2').show()\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------+----------+\n",
            "| id|  name|   datekey|\n",
            "+---+------+----------+\n",
            "| A2|Garvik|2021-06-20|\n",
            "| A1| Jugal|2021-06-20|\n",
            "| A3|  Jack|2021-06-21|\n",
            "+---+------+----------+\n",
            "\n",
            "+------------------+\n",
            "|         partition|\n",
            "+------------------+\n",
            "|datekey=2021-06-20|\n",
            "|datekey=2021-06-21|\n",
            "+------------------+\n",
            "\n",
            "+---+------+----------+\n",
            "| id|  name|   datekey|\n",
            "+---+------+----------+\n",
            "| A2|Garvik|2021-06-20|\n",
            "| A1| Jugal|2021-06-20|\n",
            "| A3|  Jack|2021-06-21|\n",
            "| A5|  Paul|2021-06-22|\n",
            "| A4|  Jill|2021-06-22|\n",
            "+---+------+----------+\n",
            "\n",
            "+------------------+\n",
            "|         partition|\n",
            "+------------------+\n",
            "|datekey=2021-06-20|\n",
            "|datekey=2021-06-21|\n",
            "|datekey=2021-06-22|\n",
            "+------------------+\n",
            "\n",
            "+---+------+----------+\n",
            "| id|  name|   datekey|\n",
            "+---+------+----------+\n",
            "| A2|Garvik|2021-06-20|\n",
            "| A1| Jugal|2021-06-20|\n",
            "| A3|  Jack|2021-06-21|\n",
            "| A6|  Mark|2021-06-22|\n",
            "+---+------+----------+\n",
            "\n",
            "+------------------+\n",
            "|         partition|\n",
            "+------------------+\n",
            "|datekey=2021-06-20|\n",
            "|datekey=2021-06-21|\n",
            "|datekey=2021-06-22|\n",
            "+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOZl57yA4Rc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1ff635a-d1aa-4862-9f68-6fc2bb73ab72"
      },
      "source": [
        " # Case1: Create a table from a dataframe - A datekey column have to be in the dataframe.\n",
        " # Case2: Create a table from files - A datekey is not required to be in the file.\n",
        "df_temp_tbl2 = spark.sql('select * from temp_tbl2')\n",
        "df_temp_tbl2.show()\n",
        "\n",
        "df_temp_tbl2.createOrReplaceTempView(\"temp_tbl3\")\n",
        "\n",
        "spark.sql('drop table if exists temp_tbl4')\n",
        "spark.sql(\"create table temp_tbl4(id string, name string) partitioned by(datekey string) stored as textfile\")\n",
        "\n",
        "# It's required to create a table first.\n",
        "# It won't write a datekey in the physical files along with data.\n",
        "spark.sql(\"insert overwrite table temp_tbl4 partition(datekey) select * from temp_tbl3\")\n",
        "\n",
        "spark.sql(\"select * from temp_tbl4\").show()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------+----------+\n",
            "| id|  name|   datekey|\n",
            "+---+------+----------+\n",
            "| A2|Garvik|2021-06-20|\n",
            "| A1| Jugal|2021-06-20|\n",
            "| A3|  Jack|2021-06-21|\n",
            "| A6|  Mark|2021-06-22|\n",
            "+---+------+----------+\n",
            "\n",
            "+---+------+----------+\n",
            "| id|  name|   datekey|\n",
            "+---+------+----------+\n",
            "| A2|Garvik|2021-06-20|\n",
            "| A1| Jugal|2021-06-20|\n",
            "| A3|  Jack|2021-06-21|\n",
            "| A6|  Mark|2021-06-22|\n",
            "+---+------+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obP01hFERf9m"
      },
      "source": [
        "###SQL-Hive Catalog"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3p0Li8_RD_1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f1a94c8-3380-4171-e619-469ea15726d9"
      },
      "source": [
        "spark.catalog.listDatabases()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Database(name='default', description='Default Hive database', locationUri='file:/content/spark-warehouse')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3sVKxZ-gttZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6aba028a-219f-4628-8cab-cfeb4ae13f39"
      },
      "source": [
        "spark.catalog.listTables()\n",
        "# temp_tbl1, temp_tbl2, temp_tbl4 -> spark.sql(create external table or create table)\n",
        "\n",
        "# people_tbl -> HiveContext.registerDataFrameAsTable, \n",
        "# people_json_tbl -> dataframe.registerTempTable\n",
        "# temp_tbl3 -> dataframe.createOrReplaceTempView"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Table(name='temp_tbl1', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
              " Table(name='temp_tbl2', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
              " Table(name='temp_tbl4', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
              " Table(name='temp_tbl41', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
              " Table(name='people_json_tbl', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
              " Table(name='people_tbl', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
              " Table(name='temp_tbl3', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7YKirvEhJRA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6193bb8a-f84f-45f8-ce0b-d7c582bdf266"
      },
      "source": [
        "spark.read.table(\"default.temp_tbl1\")"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: string, name: string, datekey: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vqkY8NAG1hS"
      },
      "source": [
        "Practices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NMPkTNdG30A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b510410-e27b-4cd9-92fa-d274c7a61bbd"
      },
      "source": [
        "# Get Max partition from the table\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "partition_df = spark.sql(\"select * from temp_tbl4\")\n",
        "partition_df.show()\n",
        "\n",
        "partition_df = spark.sql(\"show partitions temp_tbl4\")\n",
        "partition_df.show()\n",
        "\n",
        "partition_df.agg(F.max(partition_df.partition)).show()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------+----------+\n",
            "| id|  name|   datekey|\n",
            "+---+------+----------+\n",
            "| A3|  Jack|2021-06-21|\n",
            "| A6|  Mark|2021-06-22|\n",
            "| A2|Garvik|2021-06-20|\n",
            "| A1| Jugal|2021-06-20|\n",
            "+---+------+----------+\n",
            "\n",
            "+------------------+\n",
            "|         partition|\n",
            "+------------------+\n",
            "|datekey=2021-06-20|\n",
            "|datekey=2021-06-21|\n",
            "|datekey=2021-06-22|\n",
            "+------------------+\n",
            "\n",
            "+------------------+\n",
            "|    max(partition)|\n",
            "+------------------+\n",
            "|datekey=2021-06-22|\n",
            "+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7tnd8OrsnE2"
      },
      "source": [
        "### If we need to connect Hadoop Yarn Hive then make sure the script is submited in the Yarn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tolKXsfxo-PJ"
      },
      "source": [
        "# Set master as yarn in that case and enable hive support\n",
        "spark = SparkSession.builder \\\n",
        "        .master(\"yarn\") \\\n",
        "        .appName(\"Spark_App1\") \\\n",
        "        .enableHiveSupport() \\\n",
        "        .getOrCreate()"
      ],
      "execution_count": 62,
      "outputs": []
    }
  ]
}