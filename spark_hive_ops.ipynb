{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spark_hive_ops.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP0us9J1Z3gCg9yBUkrb+Yd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jugalpanchal/bd-chef/blob/main/spark_hive_ops.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l71kit82f_ac"
      },
      "source": [
        "# Follow the steps to install the dependencies:\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # install java\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz # spark package download\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz # unzip spark package\n",
        "!pip install -q findspark # install spark\n",
        "\n",
        "# Set the location of Java and Spark:\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANgvtS-DgCtw"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# create or get spark session\n",
        "# If we don't specifiy the Hive support then it gives an error while creating a table:\n",
        "# AnalysisException: Hive support is required to CREATE Hive TABLE (AS SELECT);\n",
        "spark = SparkSession.builder \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .appName(\"Spark_App1\") \\\n",
        "        .enableHiveSupport() \\\n",
        "        .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Q9zUndRgGXd"
      },
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql import HiveContext\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "hc = HiveContext(sc) # create a Hive context\n",
        "sqc = SQLContext(sc) # create a SQL context\n",
        "\n",
        "# So does diff between these two? - \n",
        "# HiveContext is still the superset of SQLContext.\n",
        "# It contains certain extra properties such as it can read the configuration from hive-site.xml, in case you have hive use cases otherwise simply use SQLContext\n",
        "\n",
        "# The sql is an alternative of the DataFrame DSL(Domain Specfic Language ex Map, GroupBy, FlatMap etc.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K13Th5iWtjMG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31ebb8b0-a8a4-4bb3-9e1e-2fba39617ae9"
      },
      "source": [
        "hc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.context.HiveContext at 0x7fb3acf95990>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-SeMAcStlw0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eabfd7ab-81ad-4b0b-a7de-bedfc12593f9"
      },
      "source": [
        "sqc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.context.SQLContext at 0x7fb3acf95a50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaubDxXwmWnO"
      },
      "source": [
        "### Load Constant Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ev9O57Q7garT",
        "outputId": "32143989-78a6-43ab-a835-67790b7e8ded"
      },
      "source": [
        "people_list = [('A', 25), ('B', 20), ('C', 30), ('D', 15)]\n",
        "people_rdd_pair = sc.parallelize(people_list) # create a RDD but with Tuple/Pair objects\n",
        "#people_rdd_pair # ParallelCollectionRDD[2] at readRDDFromFile\n",
        "\n",
        "people_rdd_row = people_rdd_pair.map(lambda x: Row(name=x[0], age=int(x[1]))) # create a RDD with Row objects\n",
        "#people # PythonRDD[5] at RDD\n",
        "\n",
        "people_rdd_row.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(name='A', age=25),\n",
              " Row(name='B', age=20),\n",
              " Row(name='C', age=30),\n",
              " Row(name='D', age=15)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MNk8Cg1jJKS",
        "outputId": "0e2c8777-81b6-43ca-a6a6-2b87d71b5d6e"
      },
      "source": [
        "people_df = spark.createDataFrame(people_rdd_row)\n",
        "#people_df # DataFrame[name: string, age: bigint]\n",
        "\n",
        "people_df.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(name='A', age=25),\n",
              " Row(name='B', age=20),\n",
              " Row(name='C', age=30),\n",
              " Row(name='D', age=15)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOJS3tpis6SB"
      },
      "source": [
        "### HiveContext"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8SdL9u1j2wr",
        "outputId": "94bd5cfb-889f-4b75-a4b2-fdaedc881681"
      },
      "source": [
        "hc.registerDataFrameAsTable(people_df, 'people_tbl') # create a table\n",
        "hc.sql('show tables').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+----------+-----------+\n",
            "|database| tableName|isTemporary|\n",
            "+--------+----------+-----------+\n",
            "| default| temp_tbl1|      false|\n",
            "| default| temp_tbl2|      false|\n",
            "| default| temp_tbl4|      false|\n",
            "|        |people_tbl|       true|\n",
            "+--------+----------+-----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIFqPgBRljvy",
        "outputId": "9abcb955-c34f-4bf3-cdfc-b55020df23cb"
      },
      "source": [
        "hc.sql('select * from people_tbl').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+---+\n",
            "|name|age|\n",
            "+----+---+\n",
            "|   A| 25|\n",
            "|   B| 20|\n",
            "|   C| 30|\n",
            "|   D| 15|\n",
            "+----+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vB6YVD5mO5d"
      },
      "source": [
        "###Load JSON Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1V39qcM4msDc",
        "outputId": "54c98da1-d143-4aa2-da24-d95bb3319666"
      },
      "source": [
        "people_json_df = spark.read.json('sample_data/anscombe.json') # Series is not Pandas class here :) It is a key in JSON file.\n",
        "people_json_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Series: string, X: double, Y: double, _corrupt_record: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkR900tZnnqj",
        "outputId": "90a90388-e4c9-4a91-ae96-2055b1e0c873"
      },
      "source": [
        "people_json_df.registerTempTable('people_json_tbl')\n",
        "hc.sql('show tables').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+---------------+-----------+\n",
            "|database|      tableName|isTemporary|\n",
            "+--------+---------------+-----------+\n",
            "| default|      temp_tbl1|      false|\n",
            "| default|      temp_tbl2|      false|\n",
            "| default|      temp_tbl4|      false|\n",
            "|        |people_json_tbl|       true|\n",
            "|        |     people_tbl|       true|\n",
            "+--------+---------------+-----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dG67LtSLn0gu",
        "outputId": "6f7bdf6c-fdb6-47dd-a230-4554e827cad0"
      },
      "source": [
        "results = hc.sql('select * from people_json_tbl')\n",
        "results.show() # default shows top 20 rows"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+----+-----+---------------+\n",
            "|Series|   X|    Y|_corrupt_record|\n",
            "+------+----+-----+---------------+\n",
            "|  null|null| null|              [|\n",
            "|     I|10.0| 8.04|           null|\n",
            "|     I| 8.0| 6.95|           null|\n",
            "|     I|13.0| 7.58|           null|\n",
            "|     I| 9.0| 8.81|           null|\n",
            "|     I|11.0| 8.33|           null|\n",
            "|     I|14.0| 9.96|           null|\n",
            "|     I| 6.0| 7.24|           null|\n",
            "|     I| 4.0| 4.26|           null|\n",
            "|     I|12.0|10.84|           null|\n",
            "|     I| 7.0| 4.81|           null|\n",
            "|     I| 5.0| 5.68|           null|\n",
            "|    II|10.0| 9.14|           null|\n",
            "|    II| 8.0| 8.14|           null|\n",
            "|    II|13.0| 8.74|           null|\n",
            "|    II| 9.0| 8.77|           null|\n",
            "|    II|11.0| 9.26|           null|\n",
            "|    II|14.0|  8.1|           null|\n",
            "|    II| 6.0| 6.13|           null|\n",
            "|    II| 4.0|  3.1|           null|\n",
            "+------+----+-----+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7tnd8OrsnE2"
      },
      "source": [
        "### If we need to connect Hadoop Yarn Hive then make sure the script is submited in the Yarn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tolKXsfxo-PJ"
      },
      "source": [
        "# Set master as yarn in that case and enable hive support\n",
        "spark = SparkSession.builder \\\n",
        "        .master(\"yarn\") \\\n",
        "        .appName(\"Spark_App1\") \\\n",
        "        .enableHiveSupport() \\\n",
        "        .getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw1UU5aBsq3T"
      },
      "source": [
        "### SQLContext"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tI8dlwJctL8S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23e32018-a2fc-46e0-e5e2-35e1abc0d79f"
      },
      "source": [
        "# It's a alternative way to play with data. SQLContext helps to create a DataFrame too.\n",
        "\n",
        "df = sqc.range(5)\n",
        "#df # DataFrame[id: bigint]\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  0|\n",
            "|  1|\n",
            "|  2|\n",
            "|  3|\n",
            "|  4|\n",
            "+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRtO_gffkKal"
      },
      "source": [
        "### Partition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1N1u3YqCqFKo",
        "outputId": "0de1278d-2b2b-486e-a4dc-886f1508c4a7"
      },
      "source": [
        "# 1. It won't delete the files and folders. because it is an external table.\n",
        "# Table or view 'temp_tbl1' already exists in database 'default'\n",
        "spark.sql('drop table if exists temp_tbl1')\n",
        "# 2. It creates a spark-warehouse/ext/temp_db/temp_tbl2/datekey=2021-06-20/_SUCCESS\n",
        "# CREATE EXTERNAL TABLE must be accompanied by LOCATION\n",
        "spark.sql(\"create external table temp_tbl1(id string, name string) partitioned by(datekey string) stored as parquet location 'ext/temp_db/temp_tbl2'\")\n",
        "\n",
        " # 3. I ran this query 5+ times. :) It keeps adding new row because the drop table does not delete existing data due to the external table.\n",
        "spark.sql(\"insert into temp_tbl1 values('A1', 'Phill', '2020-01-30')\")\n",
        "\n",
        "spark.sql('select * from temp_tbl1').show()\n",
        "spark.sql('show partitions temp_tbl1').show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-----+----------+\n",
            "| id| name|   datekey|\n",
            "+---+-----+----------+\n",
            "| A1|Phill|2020-01-30|\n",
            "| A1|Phill|2020-01-30|\n",
            "| A1|Phill|2020-01-30|\n",
            "| A1|Phill|2020-01-30|\n",
            "+---+-----+----------+\n",
            "\n",
            "+------------------+\n",
            "|         partition|\n",
            "+------------------+\n",
            "|datekey=2020-01-30|\n",
            "+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vg6whMFKkKlw",
        "outputId": "925acf9a-5b4f-4507-b2d8-c5b2dfd22e5c"
      },
      "source": [
        "# 1. It creates a metastore_db and derby.log\n",
        "# Table or view 'temp_tbl2' already exists in database 'default'\n",
        "spark.sql('drop table if exists temp_tbl2') # It drops the table and delete the table folder too\n",
        "# 2. It creates a spark-warehouse/temp_tbl2/datekey=2021-06-20/_SUCCESS\n",
        "spark.sql(\"create table temp_tbl2(id string, name string) partitioned by(datekey string) stored as parquet\")\n",
        "# 3. It creates a partition automatically and adds parquet files. The last param value is considered as a partition value.\n",
        "spark.sql(\"insert into temp_tbl2 values('A1', 'Jugal', '2021-06-20')\") # It adds a part-00000-GUID.c000.snappy.parquet file.\n",
        "spark.sql(\"insert into temp_tbl2 values('A2', 'Garvik', '2021-06-20')\") # It adds an another part-00000-GUID.c000.snappy.parquet file.\n",
        "spark.sql(\"insert into temp_tbl2 values('A3', 'Jack', '2021-06-21')\") # It adds an another part-00000-GUID.c000.snappy.parquet file but in a new partition.\n",
        "\n",
        "spark.sql('select * from temp_tbl2').show() # DataFrame\n",
        "spark.sql('show partitions temp_tbl2').show()\n",
        "\n",
        "# 4 It adds new partition\n",
        "spark.sql(\"insert overwrite table temp_tbl2 partition(datekey) values('A4', 'Jill', '2021-06-22')\")\n",
        "spark.sql(\"insert into temp_tbl2 values('A5', 'Paul', '2021-06-22')\")\n",
        "spark.sql('select * from temp_tbl2').show()\n",
        "spark.sql('show partitions temp_tbl2').show()\n",
        "\n",
        "# 5 It adds new partition but overwrite the existing partition too.\n",
        "spark.sql(\"insert overwrite table temp_tbl2 partition(datekey) values('A6', 'Mark', '2021-06-22')\")\n",
        "spark.sql('select * from temp_tbl2').show()\n",
        "spark.sql('show partitions temp_tbl2').show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------+----------+\n",
            "| id|  name|   datekey|\n",
            "+---+------+----------+\n",
            "| A2|Garvik|2021-06-20|\n",
            "| A1| Jugal|2021-06-20|\n",
            "| A3|  Jack|2021-06-21|\n",
            "+---+------+----------+\n",
            "\n",
            "+------------------+\n",
            "|         partition|\n",
            "+------------------+\n",
            "|datekey=2021-06-20|\n",
            "|datekey=2021-06-21|\n",
            "+------------------+\n",
            "\n",
            "+---+------+----------+\n",
            "| id|  name|   datekey|\n",
            "+---+------+----------+\n",
            "| A2|Garvik|2021-06-20|\n",
            "| A1| Jugal|2021-06-20|\n",
            "| A3|  Jack|2021-06-21|\n",
            "| A5|  Paul|2021-06-22|\n",
            "| A4|  Jill|2021-06-22|\n",
            "+---+------+----------+\n",
            "\n",
            "+------------------+\n",
            "|         partition|\n",
            "+------------------+\n",
            "|datekey=2021-06-20|\n",
            "|datekey=2021-06-21|\n",
            "|datekey=2021-06-22|\n",
            "+------------------+\n",
            "\n",
            "+---+------+----------+\n",
            "| id|  name|   datekey|\n",
            "+---+------+----------+\n",
            "| A2|Garvik|2021-06-20|\n",
            "| A1| Jugal|2021-06-20|\n",
            "| A3|  Jack|2021-06-21|\n",
            "| A6|  Mark|2021-06-22|\n",
            "+---+------+----------+\n",
            "\n",
            "+------------------+\n",
            "|         partition|\n",
            "+------------------+\n",
            "|datekey=2021-06-20|\n",
            "|datekey=2021-06-21|\n",
            "|datekey=2021-06-22|\n",
            "+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOZl57yA4Rc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee016e7f-f41b-4dad-cc14-477b7c05d624"
      },
      "source": [
        "df_temp_tbl2 = spark.sql('select * from temp_tbl2')\n",
        "df_temp_tbl2.createOrReplaceTempView(\"temp_tbl3\")\n",
        "\n",
        "spark.sql('drop table if exists temp_tbl4')\n",
        "spark.sql(\"create table temp_tbl4(id string, name string) partitioned by(datekey string) stored as parquet\")\n",
        "\n",
        "spark.sql(\"insert overwrite table temp_tbl4 partition(datekey) select * from temp_tbl3\") # It's required to create a table first.\n",
        "\n",
        "spark.sql(\"select * from temp_tbl4\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------+----------+\n",
            "| id|  name|   datekey|\n",
            "+---+------+----------+\n",
            "| A3|  Jack|2021-06-21|\n",
            "| A6|  Mark|2021-06-22|\n",
            "| A2|Garvik|2021-06-20|\n",
            "| A1| Jugal|2021-06-20|\n",
            "+---+------+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obP01hFERf9m"
      },
      "source": [
        "###SQL-Hive Catalog"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3p0Li8_RD_1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed438ef2-5e1c-4095-b788-ddcb85fb29dc"
      },
      "source": [
        "spark.catalog.listDatabases()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Database(name='default', description='Default Hive database', locationUri='file:/content/spark-warehouse')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    }
  ]
}