{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spark_fire.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyONQQGP5f9u/FjWVyiPiYVF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jugalpanchal/bd-chef/blob/main/spark_fire.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0jjDhKPuxhN"
      },
      "source": [
        "To run spark in Colab, we need to first install all the dependencies in Colab environment i.e. Apache Spark 2.3.2 with hadoop 2.7, Java 8 and Findspark to locate the spark in the system.\n",
        "The tools installation can be carried out inside the Jupyter Notebook of the Colab.\n",
        "Note: Spark 2.4.0 version compatibility issue with python. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_Ncmu0puy-0"
      },
      "source": [
        "# Follow the steps to install the dependencies:\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # install java\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz # spark package download\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz # unzip spark package\n",
        "!pip install -q findspark # install spark\n",
        "\n",
        "#Note: There is a possibility the location is changed or new version is introduced. So make sure the url is valid."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gld7EzbD7XDL"
      },
      "source": [
        "Set the environment path which enables us to run Pyspark in the Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVleUcML7X6Z"
      },
      "source": [
        "# Set the location of Java and Spark:\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKrUGZ417cP2"
      },
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5CwYH6P73sZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "e1d34e91-edb7-42f5-f394-f4c6d3e8f9ad"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# create or get a spark session as a spark variable\n",
        "spark = SparkSession.builder \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .appName(\"Spark_App1\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "# 'spark': <pyspark.sql.session.SparkSession at 0x7ff39867ed10>\n",
        "spark # the spark can be used for higher level api."
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://dc7075a2ad8c:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Spark_App1</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f4f68f2be90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQ2kPP1u75xy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "245cebae-068a-4f33-c029-386435d6e493"
      },
      "source": [
        "# Spark 2.x does not give direct sparkcontext so we need to get it from SparkSession\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# 'sc': <SparkContext master=local[*] appName=Spark_App1>,\n",
        "sc # the sc can be used for lower level api."
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://dc7075a2ad8c:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Spark_App1</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ],
            "text/plain": [
              "<SparkContext master=local[*] appName=Spark_App1>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2U37X_7aygs"
      },
      "source": [
        "#https://colab.research.google.com/github/asifahmed90/pyspark-ML-in-Colab/blob/master/PySpark_Regression_Analysis.ipynb\n",
        "#https://colab.research.google.com/github/jmbanda/BigDataProgramming_2019/blob/master/Chapter_5_Loading_and_Saving_Data_in_Spark.ipynb\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPM84iV1UC7u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0808a3e3-5f5b-43fd-8db8-1427784de2d4"
      },
      "source": [
        "globals() # objects are created.\n",
        "locals()\n",
        "vars()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'In': ['',\n",
              "  \"# Follow the steps to install the dependencies:\\nget_ipython().system('apt-get install openjdk-8-jdk-headless -qq > /dev/null # install java')\\nget_ipython().system('wget -q https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz # spark package download')\\nget_ipython().system('tar xf spark-3.1.2-bin-hadoop3.2.tgz # unzip spark package')\\nget_ipython().system('pip install -q findspark # install spark')\\n\\n#Note: There is a possibility the location is changed or new version is introduced. So make sure the url is valid.\",\n",
              "  '# Set the location of Java and Spark:\\nimport os\\nos.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\\nos.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"',\n",
              "  'import findspark\\nfindspark.init()',\n",
              "  'from pyspark.sql import SparkSession\\n\\n# create or get a spark session as a spark variable\\nspark = SparkSession.builder         .master(\"local[*]\")         .appName(\"Spark_App1\")         .getOrCreate()\\n\\n# \\'spark\\': <pyspark.sql.session.SparkSession at 0x7ff39867ed10>\\nspark # the spark can be used for higher level api.',\n",
              "  \"# Spark 2.x does not give direct sparkcontext so we need to get it from SparkSession\\nsc = spark.sparkContext\\n\\n# 'sc': <SparkContext master=local[*] appName=Spark_App1>,\\nsc # the sc can be used for lower level api.\",\n",
              "  '#https://colab.research.google.com/github/asifahmed90/pyspark-ML-in-Colab/blob/master/PySpark_Regression_Analysis.ipynb\\n#https://colab.research.google.com/github/jmbanda/BigDataProgramming_2019/blob/master/Chapter_5_Loading_and_Saving_Data_in_Spark.ipynb',\n",
              "  'globals() # objects are created.\\nlocals()\\nvars()'],\n",
              " 'Out': {4: <pyspark.sql.session.SparkSession at 0x7f4f68f2be90>,\n",
              "  5: <SparkContext master=local[*] appName=Spark_App1>},\n",
              " 'SparkSession': pyspark.sql.session.SparkSession,\n",
              " '_': <SparkContext master=local[*] appName=Spark_App1>,\n",
              " '_4': <pyspark.sql.session.SparkSession at 0x7f4f68f2be90>,\n",
              " '_5': <SparkContext master=local[*] appName=Spark_App1>,\n",
              " '__': <pyspark.sql.session.SparkSession at 0x7f4f68f2be90>,\n",
              " '___': '',\n",
              " '__builtin__': <module 'builtins' (built-in)>,\n",
              " '__builtins__': <module 'builtins' (built-in)>,\n",
              " '__doc__': 'Automatically created module for IPython interactive environment',\n",
              " '__loader__': None,\n",
              " '__name__': '__main__',\n",
              " '__package__': None,\n",
              " '__spec__': None,\n",
              " '_dh': ['/content'],\n",
              " '_exit_code': 0,\n",
              " '_i': '#https://colab.research.google.com/github/asifahmed90/pyspark-ML-in-Colab/blob/master/PySpark_Regression_Analysis.ipynb\\n#https://colab.research.google.com/github/jmbanda/BigDataProgramming_2019/blob/master/Chapter_5_Loading_and_Saving_Data_in_Spark.ipynb',\n",
              " '_i1': '# Follow the steps to install the dependencies:\\n!apt-get install openjdk-8-jdk-headless -qq > /dev/null # install java\\n!wget -q https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz # spark package download\\n!tar xf spark-3.1.2-bin-hadoop3.2.tgz # unzip spark package\\n!pip install -q findspark # install spark\\n\\n#Note: There is a possibility the location is changed or new version is introduced. So make sure the url is valid.',\n",
              " '_i2': '# Set the location of Java and Spark:\\nimport os\\nos.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\\nos.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"',\n",
              " '_i3': 'import findspark\\nfindspark.init()',\n",
              " '_i4': 'from pyspark.sql import SparkSession\\n\\n# create or get a spark session as a spark variable\\nspark = SparkSession.builder \\\\\\n        .master(\"local[*]\") \\\\\\n        .appName(\"Spark_App1\") \\\\\\n        .getOrCreate()\\n\\n# \\'spark\\': <pyspark.sql.session.SparkSession at 0x7ff39867ed10>\\nspark # the spark can be used for higher level api.',\n",
              " '_i5': \"# Spark 2.x does not give direct sparkcontext so we need to get it from SparkSession\\nsc = spark.sparkContext\\n\\n# 'sc': <SparkContext master=local[*] appName=Spark_App1>,\\nsc # the sc can be used for lower level api.\",\n",
              " '_i6': '#https://colab.research.google.com/github/asifahmed90/pyspark-ML-in-Colab/blob/master/PySpark_Regression_Analysis.ipynb\\n#https://colab.research.google.com/github/jmbanda/BigDataProgramming_2019/blob/master/Chapter_5_Loading_and_Saving_Data_in_Spark.ipynb',\n",
              " '_i7': 'globals() # objects are created.\\nlocals()\\nvars()',\n",
              " '_ih': ['',\n",
              "  \"# Follow the steps to install the dependencies:\\nget_ipython().system('apt-get install openjdk-8-jdk-headless -qq > /dev/null # install java')\\nget_ipython().system('wget -q https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz # spark package download')\\nget_ipython().system('tar xf spark-3.1.2-bin-hadoop3.2.tgz # unzip spark package')\\nget_ipython().system('pip install -q findspark # install spark')\\n\\n#Note: There is a possibility the location is changed or new version is introduced. So make sure the url is valid.\",\n",
              "  '# Set the location of Java and Spark:\\nimport os\\nos.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\\nos.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"',\n",
              "  'import findspark\\nfindspark.init()',\n",
              "  'from pyspark.sql import SparkSession\\n\\n# create or get a spark session as a spark variable\\nspark = SparkSession.builder         .master(\"local[*]\")         .appName(\"Spark_App1\")         .getOrCreate()\\n\\n# \\'spark\\': <pyspark.sql.session.SparkSession at 0x7ff39867ed10>\\nspark # the spark can be used for higher level api.',\n",
              "  \"# Spark 2.x does not give direct sparkcontext so we need to get it from SparkSession\\nsc = spark.sparkContext\\n\\n# 'sc': <SparkContext master=local[*] appName=Spark_App1>,\\nsc # the sc can be used for lower level api.\",\n",
              "  '#https://colab.research.google.com/github/asifahmed90/pyspark-ML-in-Colab/blob/master/PySpark_Regression_Analysis.ipynb\\n#https://colab.research.google.com/github/jmbanda/BigDataProgramming_2019/blob/master/Chapter_5_Loading_and_Saving_Data_in_Spark.ipynb',\n",
              "  'globals() # objects are created.\\nlocals()\\nvars()'],\n",
              " '_ii': \"# Spark 2.x does not give direct sparkcontext so we need to get it from SparkSession\\nsc = spark.sparkContext\\n\\n# 'sc': <SparkContext master=local[*] appName=Spark_App1>,\\nsc # the sc can be used for lower level api.\",\n",
              " '_iii': 'from pyspark.sql import SparkSession\\n\\n# create or get a spark session as a spark variable\\nspark = SparkSession.builder \\\\\\n        .master(\"local[*]\") \\\\\\n        .appName(\"Spark_App1\") \\\\\\n        .getOrCreate()\\n\\n# \\'spark\\': <pyspark.sql.session.SparkSession at 0x7ff39867ed10>\\nspark # the spark can be used for higher level api.',\n",
              " '_oh': {4: <pyspark.sql.session.SparkSession at 0x7f4f68f2be90>,\n",
              "  5: <SparkContext master=local[*] appName=Spark_App1>},\n",
              " '_sh': <module 'IPython.core.shadowns' from '/usr/local/lib/python3.7/dist-packages/IPython/core/shadowns.py'>,\n",
              " 'exit': <IPython.core.autocall.ZMQExitAutocall at 0x7f4f7048e8d0>,\n",
              " 'findspark': <module 'findspark' from '/usr/local/lib/python3.7/dist-packages/findspark.py'>,\n",
              " 'get_ipython': <bound method InteractiveShell.get_ipython of <google.colab._shell.Shell object at 0x7f4f88bd0b90>>,\n",
              " 'os': <module 'os' from '/usr/lib/python3.7/os.py'>,\n",
              " 'quit': <IPython.core.autocall.ZMQExitAutocall at 0x7f4f7048e8d0>,\n",
              " 'sc': <SparkContext master=local[*] appName=Spark_App1>,\n",
              " 'spark': <pyspark.sql.session.SparkSession at 0x7f4f68f2be90>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nHOLAjbeTWy"
      },
      "source": [
        "from pyspark import RDD\n",
        "\n",
        "for (k, v) in locals().items():\n",
        "    if isinstance(v, RDD):\n",
        "      print(k) # it prints all RDD in the spark context"
      ],
      "execution_count": 13,
      "outputs": []
    }
  ]
}