{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spark_sql_ops.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPNmTgcdzploXgLNuXxPFdj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jugalpanchal/bd-chef/blob/main/spark_sql_ops.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2M1H0UA8LO7"
      },
      "source": [
        "# Follow the steps to install the dependencies:\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # install java\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz # spark package download\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz # unzip spark package\n",
        "!pip install -q findspark # install spark\n",
        "\n",
        "# Set the location of Java and Spark:\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoNgSScc8STj"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# create or get spark session\n",
        "spark = SparkSession.builder \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .appName(\"Spark_App1\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext # create a SQL context"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN-1V6sj8VBy"
      },
      "source": [
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql.types import Row\n",
        "from datetime import datetime\n",
        "\n",
        "sqlc = SQLContext(sc) # create a SQL context\n",
        "\n",
        "# Note: we can query through the sql if the table is there. The sql cannot work directly with the dataframe. \n",
        "# We can create a table/view for the dataframe.\n",
        "# The sql returns a dataframe."
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmo9eoIO8dBp"
      },
      "source": [
        "record_rdd = sc.parallelize([Row(id = 1,\n",
        "                             name = \"Jill\",\n",
        "                             active = True,\n",
        "                             clubs = ['chess', 'hockey'], # list\n",
        "                             subjects = {\"math\": 80, 'english': 56}, # dictionary\n",
        "                             enrolled = datetime(2014, 8, 1, 14, 1, 5)),\n",
        "                         Row(id = 2,\n",
        "                             name = \"George\",\n",
        "                             active = False,\n",
        "                             clubs = ['chess', 'soccer'],\n",
        "                             subjects = {\"math\": 60, 'english': 96},\n",
        "                             enrolled = datetime(2015, 3, 21, 8, 2, 5))\n",
        "])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3Wquo-68l0e",
        "outputId": "7119be56-7bc9-4dad-94bc-e1b0bc8728a5"
      },
      "source": [
        "record_df = record_rdd.toDF()\n",
        "record_df.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------+------+---------------+--------------------+-------------------+\n",
            "| id|  name|active|          clubs|            subjects|           enrolled|\n",
            "+---+------+------+---------------+--------------------+-------------------+\n",
            "|  1|  Jill|  true|[chess, hockey]|{english -> 56, m...|2014-08-01 14:01:05|\n",
            "|  2|George| false|[chess, soccer]|{english -> 96, m...|2015-03-21 08:02:05|\n",
            "+---+------+------+---------------+--------------------+-------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mkon3t2Z84nN",
        "outputId": "504e91f5-b758-4814-ca8a-1661920323ac"
      },
      "source": [
        "# Register dataframe as a table. This is a temp table per session. As session ends then the table will be gone.\n",
        "# It does not share across the spark sessions.\n",
        "record_df.createOrReplaceTempView('record_tbl')\n",
        "\n",
        "# Once the table is created then we can use the sql context to query data from the table\n",
        "record_tbl_df1 = sqlc.sql('select * from record_tbl')\n",
        "record_tbl_df1.show()\n",
        "\n",
        "record_tbl_df2 = sqlc.sql('select id, clubs[1], subjects[\"english\"] from record_tbl')\n",
        "record_tbl_df2.show()\n",
        "\n",
        "record_tbl_df3 = sqlc.sql('select id, NOT active from record_tbl')\n",
        "record_tbl_df3.show()\n",
        "\n",
        "record_tbl_df4 = sqlc.sql('select * from record_tbl where active')\n",
        "record_tbl_df4.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------+------+---------------+--------------------+-------------------+\n",
            "| id|  name|active|          clubs|            subjects|           enrolled|\n",
            "+---+------+------+---------------+--------------------+-------------------+\n",
            "|  1|  Jill|  true|[chess, hockey]|{english -> 56, m...|2014-08-01 14:01:05|\n",
            "|  2|George| false|[chess, soccer]|{english -> 96, m...|2015-03-21 08:02:05|\n",
            "+---+------+------+---------------+--------------------+-------------------+\n",
            "\n",
            "+---+--------+-----------------+\n",
            "| id|clubs[1]|subjects[english]|\n",
            "+---+--------+-----------------+\n",
            "|  1|  hockey|               56|\n",
            "|  2|  soccer|               96|\n",
            "+---+--------+-----------------+\n",
            "\n",
            "+---+------------+\n",
            "| id|(NOT active)|\n",
            "+---+------------+\n",
            "|  1|       false|\n",
            "|  2|        true|\n",
            "+---+------------+\n",
            "\n",
            "+---+----+------+---------------+--------------------+-------------------+\n",
            "| id|name|active|          clubs|            subjects|           enrolled|\n",
            "+---+----+------+---------------+--------------------+-------------------+\n",
            "|  1|Jill|  true|[chess, hockey]|{english -> 56, m...|2014-08-01 14:01:05|\n",
            "+---+----+------+---------------+--------------------+-------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-MRQgw7_3B5",
        "outputId": "6fb09422-5eab-43e7-af66-85feeccb97c0"
      },
      "source": [
        "# It's available cross spark sessions within a cluster.\n",
        "# we have to use global_temp database/namespace.\n",
        "record_df.createOrReplaceGlobalTempView('record_glb_tbl')\n",
        "record_glb_td1 = sqlc.sql('select * from global_temp.record_glb_tbl')\n",
        "record_glb_td1.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------+------+---------------+--------------------+-------------------+\n",
            "| id|  name|active|          clubs|            subjects|           enrolled|\n",
            "+---+------+------+---------------+--------------------+-------------------+\n",
            "|  1|  Jill|  true|[chess, hockey]|{english -> 56, m...|2014-08-01 14:01:05|\n",
            "|  2|George| false|[chess, soccer]|{english -> 96, m...|2015-03-21 08:02:05|\n",
            "+---+------+------+---------------+--------------------+-------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJuQdboPXnxR"
      },
      "source": [
        "### File - Read and Write"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlKJ-bDuXq7o"
      },
      "source": [
        "# Select data from the file\n",
        "#house_df = spark.sql('select * from parquet.`/user/doc/house_dataset/file1`')\n",
        "#house_df.show()\n",
        "\n",
        "# saveAsTable()\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flXNdW3Tp49b"
      },
      "source": [
        "### Catalog"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVxdkzDap5Gi",
        "outputId": "b7f1c001-9755-46fd-def7-da6e6a9891ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "spark.catalog.listDatabases()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Database(name='default', description='default database', locationUri='file:/content/spark-warehouse')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDAjwy7oqPtr",
        "outputId": "531ba228-7b36-4c6b-c9f9-f3246d01abd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "spark.catalog.listTables()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Table(name='record_tbl', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    }
  ]
}